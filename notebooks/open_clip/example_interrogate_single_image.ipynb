{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "totvcISyQowm"
   },
   "source": [
    "# Interrogate Notebook\n",
    "> Make sure you put the .txt file in wordlists folder \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Platform Check GPU (CUDA) or CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    print ('[WARNING] CUDA/GPU is not available! Compute-intensive scripts on this notebook will be run on CPU.')\n",
    "    device =  \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if 'COLAB_GPU' in os.environ:\n",
    "    print(\"Environment is colab\")\n",
    "elif 'KAGGLE_URL_BASE' in os.environ:\n",
    "    print(\"Environment is kaggle\")\n",
    "else:\n",
    "    print(\"Environment is local\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "7JSoWTxi6I1X"
   },
   "outputs": [],
   "source": [
    "#@title Imports\n",
    "try : \n",
    "  import open_clip\n",
    "except ImportError:\n",
    "  !pip install open_clip_torch\n",
    "  import open_clip\n",
    "\n",
    "try:\n",
    "  import torch\n",
    "except ImportError:\n",
    "  !pip install torch>=1.9.0\n",
    "  import torch\n",
    "\n",
    "try:\n",
    "  import PIL\n",
    "  from PIL import Image\n",
    "except:\n",
    "  !pip install pillow\n",
    "  import PIL\n",
    "  from PIL import Image\n",
    "\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "-xXR9dNYOA93"
   },
   "outputs": [],
   "source": [
    "#@title Load wordlist file function\n",
    "def load_list(filename):\n",
    "    with open(filename, 'r', encoding='utf-8', errors='replace') as f:\n",
    "        items = [line.strip() for line in f.readlines()]\n",
    "    return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Ljp78LeUPdr_"
   },
   "outputs": [],
   "source": [
    "#@title Interrogate Single Image Function\n",
    "def interrogate_single_image(model  , \n",
    "                             preprocess,\n",
    "                             image_file = None, \n",
    "                             text_file = None ,\n",
    "                           ):\n",
    "  cwd_path = pathlib.Path.cwd().parent\n",
    "  if image_file is None :\n",
    "\n",
    "    image_file = cwd_path / 'images' / 'download_blue.png'\n",
    "  \n",
    "  if text_file is None :\n",
    "    text_file = cwd_path / 'wordlists' / 'wordlist.txt'\n",
    "\n",
    "  image = preprocess(Image.open(image_file)).unsqueeze(0)\n",
    "  txt_list = load_list(text_file)\n",
    "  text = open_clip.tokenize(txt_list)\n",
    "  \n",
    "  with torch.no_grad():\n",
    "      image_features = model.encode_image(image)\n",
    "      text_features = model.encode_text(text)\n",
    "      image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "      text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "      text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "      index = text_probs.argmax()\n",
    "      return txt_list[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_dWOGDCxR9DI",
    "outputId": "3285652d-e1cf-43e7-ec6f-c815ea6d7dc9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('RN50', 'openai'),\n",
       " ('RN50', 'yfcc15m'),\n",
       " ('RN50', 'cc12m'),\n",
       " ('RN50-quickgelu', 'openai'),\n",
       " ('RN50-quickgelu', 'yfcc15m'),\n",
       " ('RN50-quickgelu', 'cc12m'),\n",
       " ('RN101', 'openai'),\n",
       " ('RN101', 'yfcc15m'),\n",
       " ('RN101-quickgelu', 'openai'),\n",
       " ('RN101-quickgelu', 'yfcc15m'),\n",
       " ('RN50x4', 'openai'),\n",
       " ('RN50x16', 'openai'),\n",
       " ('RN50x64', 'openai'),\n",
       " ('ViT-B-32', 'openai'),\n",
       " ('ViT-B-32', 'laion400m_e31'),\n",
       " ('ViT-B-32', 'laion400m_e32'),\n",
       " ('ViT-B-32', 'laion2b_e16'),\n",
       " ('ViT-B-32', 'laion2b_s34b_b79k'),\n",
       " ('ViT-B-32-quickgelu', 'openai'),\n",
       " ('ViT-B-32-quickgelu', 'laion400m_e31'),\n",
       " ('ViT-B-32-quickgelu', 'laion400m_e32'),\n",
       " ('ViT-B-16', 'openai'),\n",
       " ('ViT-B-16', 'laion400m_e31'),\n",
       " ('ViT-B-16', 'laion400m_e32'),\n",
       " ('ViT-B-16-plus-240', 'laion400m_e31'),\n",
       " ('ViT-B-16-plus-240', 'laion400m_e32'),\n",
       " ('ViT-L-14', 'openai'),\n",
       " ('ViT-L-14', 'laion400m_e31'),\n",
       " ('ViT-L-14', 'laion400m_e32'),\n",
       " ('ViT-L-14', 'laion2b_s32b_b82k'),\n",
       " ('ViT-L-14-336', 'openai'),\n",
       " ('ViT-H-14', 'laion2b_s32b_b79k'),\n",
       " ('ViT-g-14', 'laion2b_s12b_b42k')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Choose one of these models :: model name and pretrained \n",
    "open_clip.list_pretrained()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "8tDhy38HRNX9"
   },
   "outputs": [],
   "source": [
    "# Make sure you choosed the desired model\n",
    "model, _, preprocess = open_clip.create_model_and_transforms('ViT-L/14',pretrained='openai')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "CR1qX70iSv2-",
    "outputId": "0bda18bc-6477-45da-ea1f-ef59f04eaf76"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'pixel art'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run the interrogate_single_image function \n",
    "# returns ==> the most matching text in the input text\n",
    "interrogate_single_image(model = model , preprocess=preprocess)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
