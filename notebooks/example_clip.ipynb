{"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Installing open-clip library","metadata":{"id":"j3kJ9LYUDdBI"}},{"cell_type":"code","source":"%%capture\n!pip install open_clip_torch","metadata":{"id":"svnTt4UDDhpd","execution":{"iopub.status.busy":"2023-04-13T22:10:52.170008Z","iopub.execute_input":"2023-04-13T22:10:52.171132Z","iopub.status.idle":"2023-04-13T22:11:05.502394Z","shell.execute_reply.started":"2023-04-13T22:10:52.171057Z","shell.execute_reply":"2023-04-13T22:11:05.500945Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"markdown","source":"# Cloning GitHub Repository","metadata":{"id":"7BHChZPND1e6"}},{"cell_type":"code","source":"%%capture\n!git clone https://github.com/kk-digital/kcg-ml.git","metadata":{"id":"CiJnUAniDtQc","execution":{"iopub.status.busy":"2023-04-13T22:11:05.505502Z","iopub.execute_input":"2023-04-13T22:11:05.505997Z","iopub.status.idle":"2023-04-13T22:11:08.765283Z","shell.execute_reply.started":"2023-04-13T22:11:05.505951Z","shell.execute_reply":"2023-04-13T22:11:08.763655Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"markdown","source":"# OpenClip Examples","metadata":{"id":"BSCEnTPkDT3N"}},{"cell_type":"markdown","source":"## Imports","metadata":{"id":"cIu2_l57FIHF"}},{"cell_type":"code","source":"import torch\nfrom PIL import Image\nimport open_clip","metadata":{"id":"lSb1Y61fEyXi","execution":{"iopub.status.busy":"2023-04-13T22:11:08.768505Z","iopub.execute_input":"2023-04-13T22:11:08.769832Z","iopub.status.idle":"2023-04-13T22:11:08.781675Z","shell.execute_reply.started":"2023-04-13T22:11:08.769783Z","shell.execute_reply":"2023-04-13T22:11:08.775535Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"markdown","source":"## Listing All Pre-Trained Models.","metadata":{"id":"9fdns71CFEG4"}},{"cell_type":"code","source":"open_clip.list_pretrained()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IaV34hG4E0tK","outputId":"ac7a804f-ea0b-4179-fcb9-c1a1c98cfb23","execution":{"iopub.status.busy":"2023-04-13T22:11:08.787694Z","iopub.execute_input":"2023-04-13T22:11:08.788589Z","iopub.status.idle":"2023-04-13T22:11:08.799093Z","shell.execute_reply.started":"2023-04-13T22:11:08.788533Z","shell.execute_reply":"2023-04-13T22:11:08.797948Z"},"trusted":true},"execution_count":46,"outputs":[{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"[('RN50', 'openai'),\n ('RN50', 'yfcc15m'),\n ('RN50', 'cc12m'),\n ('RN50-quickgelu', 'openai'),\n ('RN50-quickgelu', 'yfcc15m'),\n ('RN50-quickgelu', 'cc12m'),\n ('RN101', 'openai'),\n ('RN101', 'yfcc15m'),\n ('RN101-quickgelu', 'openai'),\n ('RN101-quickgelu', 'yfcc15m'),\n ('RN50x4', 'openai'),\n ('RN50x16', 'openai'),\n ('RN50x64', 'openai'),\n ('ViT-B-32', 'openai'),\n ('ViT-B-32', 'laion400m_e31'),\n ('ViT-B-32', 'laion400m_e32'),\n ('ViT-B-32', 'laion2b_e16'),\n ('ViT-B-32', 'laion2b_s34b_b79k'),\n ('ViT-B-32-quickgelu', 'openai'),\n ('ViT-B-32-quickgelu', 'laion400m_e31'),\n ('ViT-B-32-quickgelu', 'laion400m_e32'),\n ('ViT-B-16', 'openai'),\n ('ViT-B-16', 'laion400m_e31'),\n ('ViT-B-16', 'laion400m_e32'),\n ('ViT-B-16', 'laion2b_s34b_b88k'),\n ('ViT-B-16-plus-240', 'laion400m_e31'),\n ('ViT-B-16-plus-240', 'laion400m_e32'),\n ('ViT-L-14', 'openai'),\n ('ViT-L-14', 'laion400m_e31'),\n ('ViT-L-14', 'laion400m_e32'),\n ('ViT-L-14', 'laion2b_s32b_b82k'),\n ('ViT-L-14-336', 'openai'),\n ('ViT-H-14', 'laion2b_s32b_b79k'),\n ('ViT-g-14', 'laion2b_s12b_b42k'),\n ('ViT-g-14', 'laion2b_s34b_b88k'),\n ('ViT-bigG-14', 'laion2b_s39b_b160k'),\n ('roberta-ViT-B-32', 'laion2b_s12b_b32k'),\n ('xlm-roberta-base-ViT-B-32', 'laion5b_s13b_b90k'),\n ('xlm-roberta-large-ViT-H-14', 'frozen_laion5b_s13b_b90k'),\n ('convnext_base', 'laion400m_s13b_b51k'),\n ('convnext_base_w', 'laion2b_s13b_b82k'),\n ('convnext_base_w', 'laion2b_s13b_b82k_augreg'),\n ('convnext_base_w', 'laion_aesthetic_s13b_b82k'),\n ('convnext_base_w_320', 'laion_aesthetic_s13b_b82k'),\n ('convnext_base_w_320', 'laion_aesthetic_s13b_b82k_augreg'),\n ('convnext_large_d', 'laion2b_s26b_b102k_augreg'),\n ('convnext_large_d_320', 'laion2b_s29b_b131k_ft'),\n ('convnext_large_d_320', 'laion2b_s29b_b131k_ft_soup'),\n ('convnext_xxlarge', 'laion2b_s34b_b82k_augreg'),\n ('convnext_xxlarge', 'laion2b_s34b_b82k_augreg_rewind'),\n ('convnext_xxlarge', 'laion2b_s34b_b82k_augreg_soup'),\n ('coca_ViT-B-32', 'laion2b_s13b_b90k'),\n ('coca_ViT-B-32', 'mscoco_finetuned_laion2b_s13b_b90k'),\n ('coca_ViT-L-14', 'laion2b_s13b_b90k'),\n ('coca_ViT-L-14', 'mscoco_finetuned_laion2b_s13b_b90k')]"},"metadata":{}}]},{"cell_type":"markdown","source":"## Text-Image Matching Example.\nShowing the probaility distribution of a list of texts for single image.","metadata":{"id":"io_GD6wVFMft"}},{"cell_type":"code","source":"MODEL_NAME = 'ViT-L-14'\nPRETRAINED = 'laion2b_s32b_b82k'","metadata":{"id":"7YluM478FLlj","execution":{"iopub.status.busy":"2023-04-13T22:11:08.800749Z","iopub.execute_input":"2023-04-13T22:11:08.801515Z","iopub.status.idle":"2023-04-13T22:11:08.808753Z","shell.execute_reply.started":"2023-04-13T22:11:08.801479Z","shell.execute_reply":"2023-04-13T22:11:08.807661Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"model, _, preprocess = open_clip.create_model_and_transforms(model_name=MODEL_NAME, pretrained=PRETRAINED)\ntokenizer = open_clip.get_tokenizer(MODEL_NAME)\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel = model.to(device)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a8OI5XIED9nV","outputId":"b4eaa3fb-d880-4e19-95b2-f60c646a318c","execution":{"iopub.status.busy":"2023-04-13T22:11:08.810530Z","iopub.execute_input":"2023-04-13T22:11:08.811374Z","iopub.status.idle":"2023-04-13T22:11:16.114900Z","shell.execute_reply.started":"2023-04-13T22:11:08.811336Z","shell.execute_reply":"2023-04-13T22:11:16.113673Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"image = preprocess(Image.open('./kcg-ml/datasets/test_images/test_image_000.jpeg')).unsqueeze(0)\n\ntext = tokenizer([\"pixel art\", \"painting\", \"digital art\"]) # List of texts which will be compared.\n\nwith torch.no_grad():\n    image_features = model.encode_image(image)\n    text_features = model.encode_text(text)\n    image_features /= image_features.norm(dim=-1, keepdim=True)\n    text_features /= text_features.norm(dim=-1, keepdim=True)\n\n    text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n\nprint(\"Label probs:\", text_probs)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fiu4DzbZ_84z","outputId":"8a85d8b1-1cc3-4fc1-9ec2-74488470cd82","execution":{"iopub.status.busy":"2023-04-13T22:15:35.510236Z","iopub.execute_input":"2023-04-13T22:15:35.511322Z","iopub.status.idle":"2023-04-13T22:15:35.558193Z","shell.execute_reply.started":"2023-04-13T22:15:35.511279Z","shell.execute_reply":"2023-04-13T22:15:35.556760Z"},"trusted":true},"execution_count":50,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_23/2177139888.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mimage_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mtext_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mimage_features\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mimage_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/open_clip/model.py\u001b[0m in \u001b[0;36mencode_image\u001b[0;34m(self, image, normalize)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mencode_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisual\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnormalize\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/open_clip/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    467\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# shape = [*, width, grid, grid]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    470\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# shape = [*, width, grid ** 2]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# shape = [*, grid ** 2, width]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    459\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 460\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor"],"ename":"RuntimeError","evalue":"Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor","output_type":"error"}]},{"cell_type":"markdown","source":"## Getting CLIP Image Embeddings for Single Image","metadata":{"id":"NDoOGXLnGUPj"}},{"cell_type":"code","source":"with torch.no_grad():\n    image = preprocess(Image.open('./kcg-ml/datasets/test_images/test_image_000.jpeg')).unsqueeze(0).to(device)\n    emb = model.encode_image(image).cpu().detach().numpy()\n\nprint(f\"[INFO] CLIP embedding size: {emb.shape}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lVYhZVgTF9Jj","outputId":"2acaf23e-8dc4-4619-bad9-dfe1ba2a3810","execution":{"iopub.status.busy":"2023-04-13T22:11:16.525053Z","iopub.status.idle":"2023-04-13T22:11:16.525942Z","shell.execute_reply.started":"2023-04-13T22:11:16.525656Z","shell.execute_reply":"2023-04-13T22:11:16.525685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Checking the Similarity Between Two Image Using CLIP","metadata":{"id":"jT8ny-ZRHzth"}},{"cell_type":"code","source":"image1 = preprocess(Image.open('./kcg-ml/datasets/test_images/test_image_001.jpeg')).unsqueeze(0).to(device)\nimage2 = preprocess(Image.open('./kcg-ml/datasets/test_images/test_image_002.jpeg')).unsqueeze(0).to(device)\n\nimage_features = model.encode_image(image1)\nimage_2_features = model.encode_image(image2)\n\nimage_features /= image_features.norm(dim=-1, keepdim=True)\nimage_2_features /= image_2_features.norm(dim=-1, keepdim=True)\nsimilarity = image_2_features.detach() @ image_features.detach().T\nprint(f'Similarit: {similarity.cpu().detach().numpy()[0][0]:.4f}')","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XC-V-KuoGNkv","outputId":"fd8691b3-d248-4296-f5ea-405824430fcb","execution":{"iopub.status.busy":"2023-04-13T22:11:16.527607Z","iopub.status.idle":"2023-04-13T22:11:16.528425Z","shell.execute_reply.started":"2023-04-13T22:11:16.528144Z","shell.execute_reply":"2023-04-13T22:11:16.528171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ClipModel Examples\n▶ ClipModel : Module built over OpenClip function, check: https://github.com/kk-digital/kcg-ml/blob/main/examples/ClipTools.py","metadata":{"id":"IBWrN2TzJ6pR"}},{"cell_type":"code","source":"!pip install patool","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pu1mv48cLF-f","outputId":"cf5355bf-d614-4fc3-8ca6-1448fed615aa","execution":{"iopub.status.busy":"2023-04-13T22:11:16.529865Z","iopub.status.idle":"2023-04-13T22:11:16.530689Z","shell.execute_reply.started":"2023-04-13T22:11:16.530412Z","shell.execute_reply":"2023-04-13T22:11:16.530438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\nsys.path.insert(0, './kcg-ml/image_classifier_pipeline/')\nsys.path.insert(0, './kcg-ml/')\n\nfrom examples.ClipTools import ClipModel","metadata":{"id":"UPUAScgOKU76"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating an Instance of ClipModel Class","metadata":{"id":"gfeeFzODNyOK"}},{"cell_type":"code","source":"clip_model_instance = ClipModel(clip_model=MODEL_NAME, pretrained=PRETRAINED)","metadata":{"id":"KBQRePaRKoVu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Downloading Model Method Example.","metadata":{"id":"AJFDH08GN2Cf"}},{"cell_type":"code","source":"clip_model_instance.download_model(MODEL_NAME, PRETRAINED)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n4BtxteMLuRC","outputId":"ea84c0eb-a487-40ff-dee9-977a4a2bedc4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Encoding Image File Using It's Path","metadata":{"id":"qSFpfaX0ODAR"}},{"cell_type":"code","source":"emb = clip_model_instance.encode_image_from_image_file('./kcg-ml/datasets/test_images/test_image_000.jpeg')\nprint(f\"Embedding vector : {emb}\")\nprint(f\"Embedding size : {emb.shape}\")","metadata":{"id":"pbwGlM52MVef"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Encoding Image Byte Object","metadata":{}},{"cell_type":"code","source":"def image_to_bytes(image_path):\n    with open(image_path, 'rb') as image_file:\n        bytes_array = bytearray(image_file.read())\n    return bytes_array\n\nemb = clip_model_instance.encode_image_from_image_data(image_to_bytes('./kcg-ml/datasets/test_images/test_image_000.jpeg'))\nprint(f\"Embedding vector : {emb}\")\nprint(f\"Embedding size : {emb.shape}\")","metadata":{"id":"GK5fo_2vOWxV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Encoding a List of Images","metadata":{}},{"cell_type":"code","source":"emb_list = clip_model_instance.encode_image_list(['./kcg-ml/datasets/test_images/test_image_000.jpeg', './kcg-ml/datasets/test_images/test_image_001.jpeg', './kcg-ml/datasets/test_images/test_image_002.jpeg'])\nfor emb in emb_list:\n  print(f\"Embedding vector : {emb}\")\n  print(f\"Embedding size : {emb.shape}\")\n  print(\"#\"*50)","metadata":{"id":"9tU-bhtLONLh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Encoding a Dictionary of Zip Files","metadata":{}},{"cell_type":"code","source":"clip_model_instance.encode_data_directory('/content/kcg-ml/datasets/test_zip_files')","metadata":{},"execution_count":null,"outputs":[]}]}