{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installing open-clip library"
      ],
      "metadata": {
        "id": "j3kJ9LYUDdBI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install open_clip_torch"
      ],
      "metadata": {
        "id": "svnTt4UDDhpd"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cloning GitHub Repository"
      ],
      "metadata": {
        "id": "7BHChZPND1e6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!git clone https://github.com/kk-digital/kcg-ml.git"
      ],
      "metadata": {
        "id": "CiJnUAniDtQc"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OpenClip Examples"
      ],
      "metadata": {
        "id": "BSCEnTPkDT3N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "cIu2_l57FIHF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "import open_clip"
      ],
      "metadata": {
        "id": "lSb1Y61fEyXi"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Listing All Pre-Trained Models."
      ],
      "metadata": {
        "id": "9fdns71CFEG4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "open_clip.list_pretrained()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IaV34hG4E0tK",
        "outputId": "ac7a804f-ea0b-4179-fcb9-c1a1c98cfb23"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('RN50', 'openai'),\n",
              " ('RN50', 'yfcc15m'),\n",
              " ('RN50', 'cc12m'),\n",
              " ('RN50-quickgelu', 'openai'),\n",
              " ('RN50-quickgelu', 'yfcc15m'),\n",
              " ('RN50-quickgelu', 'cc12m'),\n",
              " ('RN101', 'openai'),\n",
              " ('RN101', 'yfcc15m'),\n",
              " ('RN101-quickgelu', 'openai'),\n",
              " ('RN101-quickgelu', 'yfcc15m'),\n",
              " ('RN50x4', 'openai'),\n",
              " ('RN50x16', 'openai'),\n",
              " ('RN50x64', 'openai'),\n",
              " ('ViT-B-32', 'openai'),\n",
              " ('ViT-B-32', 'laion400m_e31'),\n",
              " ('ViT-B-32', 'laion400m_e32'),\n",
              " ('ViT-B-32', 'laion2b_e16'),\n",
              " ('ViT-B-32', 'laion2b_s34b_b79k'),\n",
              " ('ViT-B-32-quickgelu', 'openai'),\n",
              " ('ViT-B-32-quickgelu', 'laion400m_e31'),\n",
              " ('ViT-B-32-quickgelu', 'laion400m_e32'),\n",
              " ('ViT-B-16', 'openai'),\n",
              " ('ViT-B-16', 'laion400m_e31'),\n",
              " ('ViT-B-16', 'laion400m_e32'),\n",
              " ('ViT-B-16', 'laion2b_s34b_b88k'),\n",
              " ('ViT-B-16-plus-240', 'laion400m_e31'),\n",
              " ('ViT-B-16-plus-240', 'laion400m_e32'),\n",
              " ('ViT-L-14', 'openai'),\n",
              " ('ViT-L-14', 'laion400m_e31'),\n",
              " ('ViT-L-14', 'laion400m_e32'),\n",
              " ('ViT-L-14', 'laion2b_s32b_b82k'),\n",
              " ('ViT-L-14-336', 'openai'),\n",
              " ('ViT-H-14', 'laion2b_s32b_b79k'),\n",
              " ('ViT-g-14', 'laion2b_s12b_b42k'),\n",
              " ('ViT-g-14', 'laion2b_s34b_b88k'),\n",
              " ('ViT-bigG-14', 'laion2b_s39b_b160k'),\n",
              " ('roberta-ViT-B-32', 'laion2b_s12b_b32k'),\n",
              " ('xlm-roberta-base-ViT-B-32', 'laion5b_s13b_b90k'),\n",
              " ('xlm-roberta-large-ViT-H-14', 'frozen_laion5b_s13b_b90k'),\n",
              " ('convnext_base', 'laion400m_s13b_b51k'),\n",
              " ('convnext_base_w', 'laion2b_s13b_b82k'),\n",
              " ('convnext_base_w', 'laion2b_s13b_b82k_augreg'),\n",
              " ('convnext_base_w', 'laion_aesthetic_s13b_b82k'),\n",
              " ('convnext_base_w_320', 'laion_aesthetic_s13b_b82k'),\n",
              " ('convnext_base_w_320', 'laion_aesthetic_s13b_b82k_augreg'),\n",
              " ('convnext_large_d', 'laion2b_s26b_b102k_augreg'),\n",
              " ('convnext_large_d_320', 'laion2b_s29b_b131k_ft'),\n",
              " ('convnext_large_d_320', 'laion2b_s29b_b131k_ft_soup'),\n",
              " ('convnext_xxlarge', 'laion2b_s34b_b82k_augreg'),\n",
              " ('convnext_xxlarge', 'laion2b_s34b_b82k_augreg_rewind'),\n",
              " ('convnext_xxlarge', 'laion2b_s34b_b82k_augreg_soup'),\n",
              " ('coca_ViT-B-32', 'laion2b_s13b_b90k'),\n",
              " ('coca_ViT-B-32', 'mscoco_finetuned_laion2b_s13b_b90k'),\n",
              " ('coca_ViT-L-14', 'laion2b_s13b_b90k'),\n",
              " ('coca_ViT-L-14', 'mscoco_finetuned_laion2b_s13b_b90k')]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text-Image Matching Example.\n",
        "Showing the probaility distribution of a list of texts for single image."
      ],
      "metadata": {
        "id": "io_GD6wVFMft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = 'ViT-L-14'\n",
        "PRETRAINED = 'openai'"
      ],
      "metadata": {
        "id": "7YluM478FLlj"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, _, preprocess = open_clip.create_model_and_transforms(model_name=MODEL_NAME, pretrained=PRETRAINED)\n",
        "tokenizer = open_clip.get_tokenizer(MODEL_NAME)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8OI5XIED9nV",
        "outputId": "b4eaa3fb-d880-4e19-95b2-f60c646a318c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|███████████████████████████████████████| 933M/933M [00:13<00:00, 68.4MiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fiu4DzbZ_84z",
        "outputId": "8a85d8b1-1cc3-4fc1-9ec2-74488470cd82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label probs: tensor([[9.9429e-01, 3.5808e-04, 5.3562e-03]])\n"
          ]
        }
      ],
      "source": [
        "image = preprocess(Image.open('./kcg-ml/datasets/test-images/example3.jpg')).unsqueeze(0)\n",
        "\n",
        "text = tokenizer([\"pixel art\", \"painting\", \"digital art\"]) # List of texts which will be compared.\n",
        "\n",
        "with torch.no_grad():\n",
        "    image_features = model.encode_image(image)\n",
        "    text_features = model.encode_text(text)\n",
        "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "    text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
        "\n",
        "print(\"Label probs:\", text_probs)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting CLIP Image Embeddings for Single Image"
      ],
      "metadata": {
        "id": "NDoOGXLnGUPj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    image = preprocess(Image.open('./kcg-ml/datasets/test-images/example3.jpg')).unsqueeze(0).to(device)\n",
        "    emb = model.encode_image(image).cpu().detach().numpy()\n",
        "\n",
        "print(f\"[INFO] CLIP embedding size: {emb.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVYhZVgTF9Jj",
        "outputId": "2acaf23e-8dc4-4619-bad9-dfe1ba2a3810"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] CLIP embedding size: (1, 768)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Checking the Similarity Between Two Image Using CLIP"
      ],
      "metadata": {
        "id": "jT8ny-ZRHzth"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image1 = preprocess(Image.open('./kcg-ml/datasets/test-images/example1.jpg')).unsqueeze(0).to(device)\n",
        "image2 = preprocess(Image.open('./kcg-ml/datasets/test-images/example2.jpg')).unsqueeze(0).to(device)\n",
        "\n",
        "image_features = model.encode_image(image1)\n",
        "image_2_features = model.encode_image(image2)\n",
        "\n",
        "image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "image_2_features /= image_2_features.norm(dim=-1, keepdim=True)\n",
        "similarity = image_2_features.detach() @ image_features.detach().T\n",
        "print(f'Similarit: {similarity.cpu().detach().numpy()[0][0]:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XC-V-KuoGNkv",
        "outputId": "fd8691b3-d248-4296-f5ea-405824430fcb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarit: 0.5437\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ClipModel Examples\n",
        "▶ ClipModel : Module built over OpenClip function, check: https://github.com/kk-digital/kcg-ml/blob/main/examples/ClipTools.py"
      ],
      "metadata": {
        "id": "IBWrN2TzJ6pR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install patool"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pu1mv48cLF-f",
        "outputId": "cf5355bf-d614-4fc3-8ca6-1448fed615aa"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting patool\n",
            "  Downloading patool-1.12-py2.py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.5/77.5 KB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: patool\n",
            "Successfully installed patool-1.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.insert(0, './kcg-ml/clip_linear_probe_pipeline/')\n",
        "sys.path.insert(0, './kcg-ml/')\n",
        "\n",
        "from examples.ClipTools import ClipModel"
      ],
      "metadata": {
        "id": "UPUAScgOKU76"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating an Instance of ClipModel Class"
      ],
      "metadata": {
        "id": "gfeeFzODNyOK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clip_model_instance = ClipModel(clip_model=MODEL_NAME, pretrained=PRETRAINED)"
      ],
      "metadata": {
        "id": "KBQRePaRKoVu"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downloading Model Method Example."
      ],
      "metadata": {
        "id": "AJFDH08GN2Cf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clip_model_instance.download_model(MODEL_NAME, PRETRAINED)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4BtxteMLuRC",
        "outputId": "ea84c0eb-a487-40ff-dee9-977a4a2bedc4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Model downloaded succesfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting Image Embedding Example."
      ],
      "metadata": {
        "id": "qSFpfaX0ODAR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clip_model_instance.encode_image_from_image_file('./kcg-ml/datasets/test-images/example1.jpg')"
      ],
      "metadata": {
        "id": "pbwGlM52MVef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def image_to_bytes(image_path):\n",
        "    with open(image_path, 'rb') as image_file:\n",
        "        bytes_array = bytearray(image_file.read())\n",
        "    return bytes_array\n",
        "\n",
        "clip_model_instance.encode_image_from_image_data(image_to_bytes('./kcg-ml/datasets/test-images/example1.jpg'))"
      ],
      "metadata": {
        "id": "GK5fo_2vOWxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clip_model_instance.encode_image_list(['./kcg-ml/datasets/test-images/example1.jpg', './kcg-ml/datasets/test-images/example2.jpg', './kcg-ml/datasets/test-images/example3.jpg'])"
      ],
      "metadata": {
        "id": "9tU-bhtLONLh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}