{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mount Google Drive\n",
    "\n",
    "Mount your google drive to be used for storing the storing the dataset into it (optional to save datasets on mega to it) and also for saving the results in the drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount._DEBUG = False\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Move Dataset from Mega to Drive\n",
    "\n",
    "Move the file with the provided link (Provide your username and password if it's not a public link) to your google drive already mounted before in the previous step. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Mega"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, urllib.request\n",
    "import time\n",
    "import subprocess\n",
    "import contextlib\n",
    "from IPython.display import clear_output\n",
    "from functools import wraps\n",
    "import errno\n",
    "import os\n",
    "import signal\n",
    "import subprocess\n",
    "import shlex\n",
    "\n",
    "HOME = os.path.expanduser(\"~\")\n",
    "if not os.path.exists(f\"{HOME}/.ipython/ocr.py\"):\n",
    "    hCode = \"https://raw.githubusercontent.com/biplobsd/\" \\\n",
    "                \"OneClickRun/master/res/ocr.py\"\n",
    "    urllib.request.urlretrieve(hCode, f\"{HOME}/.ipython/ocr.py\")\n",
    "\n",
    "from ocr import (\n",
    "    runSh,\n",
    "    loadingAn,\n",
    ")\n",
    "\n",
    "\n",
    "if not os.path.exists(\"/usr/bin/mega-cmd\"):\n",
    "    loadingAn()\n",
    "    print(\"Installing MEGA ...\")\n",
    "    runSh('sudo apt-get -y update')\n",
    "    runSh('sudo apt-get -y install libmms0 libc-ares2 libc6 libcrypto++6 libgcc1 libmediainfo0v5 libpcre3 libpcrecpp0v5 libssl1.1 libstdc++6 libzen0v5 zlib1g apt-transport-https')\n",
    "    runSh('sudo curl -sL -o /var/cache/apt/archives/MEGAcmd.deb https://mega.nz/linux/MEGAsync/Debian_9.0/amd64/megacmd-Debian_9.0_amd64.deb', output=True)\n",
    "    runSh('sudo dpkg -i /var/cache/apt/archives/MEGAcmd.deb', output=True)\n",
    "    print(\"MEGA is installed.\")\n",
    "    clear_output()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### provide URL and Mega ID\n",
    "\n",
    "Add the URL to fetch from Mega, and the username and password for those files if it wasn't a public URL and to use the pro mega quota if you have a pro account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"\" \n",
    "OUTPUT_PATH = \"\" \n",
    "USERNAME = \"\"  \n",
    "PASSWORD = \"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unix, Windows and old Macintosh end-of-line\n",
    "newlines = ['\\n', '\\r\\n', '\\r']\n",
    "\n",
    "def unbuffered(proc, stream='stdout'):\n",
    "    stream = getattr(proc, stream)\n",
    "    with contextlib.closing(stream):\n",
    "        while True:\n",
    "            out = []\n",
    "            last = stream.read(1)\n",
    "            # Don't loop forever\n",
    "            if last == '' and proc.poll() is not None:\n",
    "                break\n",
    "            while last not in newlines:\n",
    "                # Don't loop forever\n",
    "                if last == '' and proc.poll() is not None:\n",
    "                    break\n",
    "                out.append(last)\n",
    "                last = stream.read(1)\n",
    "            out = ''.join(out)\n",
    "            yield out\n",
    "\n",
    "\n",
    "def transfare():\n",
    "    import codecs\n",
    "    decoder = codecs.getincrementaldecoder(\"UTF-8\")()\n",
    "    cmd = [\"mega-get\", URL, OUTPUT_PATH]\n",
    "    proc = subprocess.Popen(\n",
    "        cmd,\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.STDOUT,\n",
    "        # Make all end-of-lines '\\n'\n",
    "        universal_newlines=True,\n",
    "    )\n",
    "    for line in unbuffered(proc):\n",
    "        print(line)\n",
    "        \n",
    "\n",
    "if not OUTPUT_PATH:\n",
    "  os.makedirs(\"downloads\", exist_ok=True)\n",
    "  OUTPUT_PATH = \"downloads\"\n",
    "\n",
    "\n",
    "class TimeoutError(Exception):\n",
    "    pass\n",
    "\n",
    "def timeout(seconds=10, error_message=os.strerror(errno.ETIME)):\n",
    "    def decorator(func):\n",
    "        def _handle_timeout(signum, frame):\n",
    "            raise TimeoutError(error_message)\n",
    "\n",
    "        def wrapper(*args, **kwargs):\n",
    "            signal.signal(signal.SIGALRM, _handle_timeout)\n",
    "            signal.alarm(seconds)\n",
    "            try:\n",
    "                result = func(*args, **kwargs)\n",
    "            finally:\n",
    "                signal.alarm(0)\n",
    "            return result\n",
    "\n",
    "        return wraps(func)(wrapper)\n",
    "\n",
    "    return decorator\n",
    "\n",
    "\n",
    "@timeout(10)\n",
    "def runShT(args):\n",
    "    return runSh(args, output=True)\n",
    "\n",
    "def login(): \n",
    "    runShT(f\"mega-login {USERNAME} {PASSWORD}\")\n",
    "    \n",
    "if not (USERNAME == \"\" or PASSWORD == \"\"):\n",
    "    try:\n",
    "        login()\n",
    "    except TimeoutError:\n",
    "        runSh('mega-whoami', output=True)\n",
    "else:\n",
    "    print(\"Please Input your Mega IDs.\")\n",
    "\n",
    "transfare()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch & Clone Repo\n",
    "\n",
    "Clone the `image-tagging-tools` repo as it has all the required utils and codes from preprocessing the image datasets to training the models and using them to classify the images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/kk-digital/image-tagging-tools.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess the dataset images (Stage 1)\n",
    "\n",
    "Use the `image-dataset-processor` utils from the previously cloned repo to process a directory of images (paths to directory of images or an archived dataset), and computes the images metadata along with its CLIP embeddings and writes the result into a JSON file into `output_folder`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install requirements "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r  ./image-tagging-tools/image-dataset-processor/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the module/Utility "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, './image-tagging-tools/image-dataset-processor/')\n",
    "\n",
    "from ImageDatasetProcessor import ImageDatasetProcessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set required variables by the utility\n",
    "\n",
    "Initialize the required parameters needed by the dataset preprocessor utility and they are described as follows: \n",
    "\n",
    "* `input_folder` _[str]_ -  path to the directory containing sub-folders of each tag.\n",
    "* `output_folder` _[str]_ - path to the directory where to save the files into it.\n",
    "\n",
    "* `clip_model` _[str]_ - CLIP model to be used\n",
    "\n",
    "* `pretrained` _[str]_ - the pre-trained model to be used for CLIP\n",
    "* `batch_size` _[int]_ -  number of images to process at a time\n",
    "* `num_threads` _[int]_ - the number to be used in this process\n",
    "\n",
    "* `device` _[str]_ -  the device to be used in computing the CLIP embeddings, if `None` is provided then `cuda` will be used if available\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"\"\n",
    "output_folder = \"\"\n",
    "clip_model = \"ViT-B-32\"\n",
    "pretrained = 'openai'\n",
    "batch_size = 32,\n",
    "num_threads = 4,\n",
    "device = None, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ImageDatasetProcessor.process_dataset(\n",
    "    dataset_path, \n",
    "    output_folder, \n",
    "    clip_model, \n",
    "    pretrained, \n",
    "    batch_size, \n",
    "    num_threads, \n",
    "    device\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b8e93f441cdd34ec3e1aab4168a471abe1992cd5890142962f4e98cab1379cc7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
