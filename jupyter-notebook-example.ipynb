{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhEVe0JFcuu_"
      },
      "source": [
        "# Mount Google Drive\n",
        "\n",
        "Mount your google drive to be used for storing the storing the dataset into it (optional to save datasets on mega to it) and also for saving the results in the drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ONxui2_ccuvK",
        "outputId": "464ce773-3a3e-4ff8-8625-4c08c0008e4b"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount._DEBUG = False\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fW5yYaLVcuvM"
      },
      "source": [
        "# Move Dataset from Mega to Drive\n",
        "\n",
        "Move the file with the provided link (Provide your username and password if it's not a public link) to your google drive already mounted before in the previous step. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dK4RNS2lcuvN"
      },
      "source": [
        "### Install Mega"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6nd_7Rg6cuvO"
      },
      "outputs": [],
      "source": [
        "import sys, os, urllib.request\n",
        "import time\n",
        "import subprocess\n",
        "import contextlib\n",
        "from IPython.display import clear_output\n",
        "from functools import wraps\n",
        "import errno\n",
        "import os\n",
        "import signal\n",
        "import subprocess\n",
        "import shlex\n",
        "import glob\n",
        "\n",
        "HOME = os.path.expanduser(\"~\")\n",
        "if not os.path.exists(f\"{HOME}/.ipython/ocr.py\"):\n",
        "    hCode = \"https://raw.githubusercontent.com/biplobsd/\" \\\n",
        "                \"OneClickRun/master/res/ocr.py\"\n",
        "    urllib.request.urlretrieve(hCode, f\"{HOME}/.ipython/ocr.py\")\n",
        "\n",
        "from ocr import (\n",
        "    runSh,\n",
        "    loadingAn,\n",
        ")\n",
        "\n",
        "if not os.path.exists(\"/usr/bin/mega-cmd\"):\n",
        "    loadingAn()\n",
        "    print(\"Installing MEGA ...\")\n",
        "    runSh('sudo apt-get -y update')\n",
        "    runSh('sudo apt-get -y install libmms0 libc-ares2 libc6 libcrypto++6 libgcc1 libmediainfo0v5 libpcre3 libpcrecpp0v5 libssl1.1 libstdc++6 libzen0v5 zlib1g apt-transport-https')\n",
        "    runSh('sudo curl -sL -o /var/cache/apt/archives/MEGAcmd.deb https://mega.nz/linux/MEGAsync/Debian_9.0/amd64/megacmd-Debian_9.0_amd64.deb', output=True)\n",
        "    runSh('sudo dpkg -i /var/cache/apt/archives/MEGAcmd.deb', output=True)\n",
        "    print(\"MEGA is installed.\")\n",
        "    clear_output()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Pi_Rk09cuvQ"
      },
      "source": [
        "### provide URL and Mega ID\n",
        "\n",
        "Add the URL to fetch from Mega, and the username and password for those files if it wasn't a public URL and to use the pro mega quota if you have a pro account."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9jy2wxACPmn"
      },
      "outputs": [],
      "source": [
        "#It's optional to provide the MEGA username and password, it's used for giving more download quota if you have a MEGA pro account. \n",
        "MEGA_USERNAME = \"\"  #optional \n",
        "MEGA_PASSWORD = \"\"  #optional \n",
        "\n",
        "TAGGED_DATASET_URL = \"https://mega.nz/file/kc40hbgL#WH0rLmRwkJodzD4yazWWAlnmp_IJQiLG0jx2hhDJLhY\"\n",
        "OTHER_DATASET_URL  = \"https://mega.nz/file/MNo0ABSB#8rDqevxdQmtaNFKjQ3RS9v2pF-jL8xzmM9LLxIfAhG0\"\n",
        "OUTPUT_PATH = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjz0-BUHcuvS",
        "outputId": "d59e26fd-0be2-4da3-e546-458c74a1a20d"
      },
      "outputs": [],
      "source": [
        "# Unix, Windows and old Macintosh end-of-line\n",
        "newlines = ['\\n', '\\r\\n', '\\r']\n",
        "\n",
        "def latest_file(folder):\n",
        "  list_of_files = glob.glob(f'{folder}/*') # * means all \n",
        "  latest_file = max(list_of_files, key=os.path.getctime)\n",
        "  return latest_file\n",
        "\n",
        "def unbuffered(proc, stream='stdout'):\n",
        "    stream = getattr(proc, stream)\n",
        "    with contextlib.closing(stream):\n",
        "        while True:\n",
        "            out = []\n",
        "            last = stream.read(1)\n",
        "            # Don't loop forever\n",
        "            if last == '' and proc.poll() is not None:\n",
        "                break\n",
        "            while last not in newlines:\n",
        "                # Don't loop forever\n",
        "                if last == '' and proc.poll() is not None:\n",
        "                    break\n",
        "                out.append(last)\n",
        "                last = stream.read(1)\n",
        "            out = ''.join(out)\n",
        "            yield out\n",
        "\n",
        "\n",
        "def transfer(url):\n",
        "    import codecs\n",
        "    decoder = codecs.getincrementaldecoder(\"UTF-8\")()\n",
        "    cmd = [\"mega-get\", url, OUTPUT_PATH]\n",
        "    proc = subprocess.Popen(\n",
        "        cmd,\n",
        "        stdout=subprocess.PIPE,\n",
        "        stderr=subprocess.STDOUT,\n",
        "        # Make all end-of-lines '\\n'\n",
        "        universal_newlines=True,\n",
        "    )\n",
        "    for line in unbuffered(proc):\n",
        "        print(line)\n",
        "        \n",
        "if not OUTPUT_PATH:\n",
        "  os.makedirs(\"downloads\", exist_ok=True)\n",
        "  OUTPUT_PATH = \"downloads\"\n",
        "\n",
        "\n",
        "class TimeoutError(Exception):\n",
        "    pass\n",
        "\n",
        "def timeout(seconds=10, error_message=os.strerror(errno.ETIME)):\n",
        "    def decorator(func):\n",
        "        def _handle_timeout(signum, frame):\n",
        "            raise TimeoutError(error_message)\n",
        "\n",
        "        def wrapper(*args, **kwargs):\n",
        "            signal.signal(signal.SIGALRM, _handle_timeout)\n",
        "            signal.alarm(seconds)\n",
        "            try:\n",
        "                result = func(*args, **kwargs)\n",
        "            finally:\n",
        "                signal.alarm(0)\n",
        "            return result\n",
        "\n",
        "        return wraps(func)(wrapper)\n",
        "\n",
        "    return decorator\n",
        "\n",
        "\n",
        "@timeout(10)\n",
        "def runShT(args):\n",
        "    return runSh(args, output=True)\n",
        "\n",
        "def login(): \n",
        "    runShT(f\"mega-login {MEGA_USERNAME} {MEGA_PASSWORD}\")\n",
        "\n",
        "#if the username and password provided then login to MEGA. \n",
        "if MEGA_USERNAME != \"\" and MEGA_PASSWORD != \"\":\n",
        "    try:\n",
        "        login()\n",
        "    except TimeoutError:\n",
        "        runSh('mega-whoami', output=True)\n",
        "else:\n",
        "    print(\"Please Input your Mega IDs.\")\n",
        "\n",
        "transfer(TAGGED_DATASET_URL)\n",
        "tagged_dataset_path = latest_file('./downloads')\n",
        "transfer(OTHER_DATASET_URL)\n",
        "other_dataset_path = latest_file('./downloads')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFI20pLVcuvU"
      },
      "source": [
        "# Fetch & Clone Repo\n",
        "\n",
        "Clone the `image-tagging-tools` repo as it has all the required utils and codes from preprocessing the image datasets to training the models and using them to classify the images. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6n6QEB49cuvV",
        "outputId": "694d6054-03f6-4fda-9836-cc6dad61811b"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/kk-digital/image-tagging-tools.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hYcmeZicuvW"
      },
      "source": [
        "# Preprocess the dataset images (Stage 1)\n",
        "\n",
        "Use the `image-dataset-processor` utils from the previously cloned repo to process a directory of images (paths to directory of images or an archived dataset), and computes the images metadata along with its CLIP embeddings and writes the result into a JSON file into `output_folder`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guwuN0UncuvW"
      },
      "source": [
        "### Install requirements "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86S-QyvSnwG8"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%pip install ascii_graph open_clip_torch patool fire"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiqXMqOEcuvX"
      },
      "source": [
        "### Import the module/Utility "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "usaH8e-fcuvY"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.insert(0, './image-tagging-tools/stage1/')\n",
        "sys.path.insert(0, './image-tagging-tools/')\n",
        "from ImageDatasetProcessor import ImageDatasetProcessor\n",
        "from classify import main as classify_main\n",
        "from train import main as train_main\n",
        "from classify_helper_functions import clean_file\n",
        "import patoolib\n",
        "import shutil\n",
        "\n",
        "\n",
        "def is_archive(path: str) -> bool:\n",
        "    \"\"\"method to check if a given path is an archive.\n",
        "    :param path: The file path to check. \n",
        "    :type path: str\n",
        "    :returns: `True` if the given path is a path of an archived file. \n",
        "    :rtype: bool\n",
        "    \"\"\"\n",
        "    \n",
        "    try: \n",
        "        patoolib.get_archive_format(path)\n",
        "        return True \n",
        "    except Exception: \n",
        "        return False \n",
        "\n",
        "def unzip_folder(folder_path :str):\n",
        "    \"\"\"takes an archived file path and unzip it.\n",
        "    :param folder_path: path to the archived file.\n",
        "    :type folder_path: str\n",
        "    :returns: path of the new exracted folder \n",
        "    :rtype: str\n",
        "    \"\"\"\n",
        "    dir_path  = os.path.dirname(folder_path)\n",
        "    file_name = os.path.basename(folder_path).split('.zip')[0]\n",
        "    os.makedirs(dir_path , exist_ok=True)\n",
        "    \n",
        "    print(\"[INFO] Extracting the archived file...\")\n",
        "    patoolib.extract_archive(folder_path, outdir=dir_path)\n",
        "    print(\"[INFO] Extraction completed.\")\n",
        "    \n",
        "    return latest_file(dir_path)\n",
        "\n",
        "def clean_directory(\n",
        "                    dir_path : str ,\n",
        "                    only_sub_dir : bool = False\n",
        "                    ):\n",
        "    \"\"\" clean a directory files and folders\n",
        "\n",
        "    :param dir_path: path to the directory which will be cleaned.\n",
        "    :type dir_path: str\n",
        "    :param only_sub_dir: an option to make it only sub directories \n",
        "                          for ex: in cleaning pixel-art-tagged folder.\n",
        "    :type only_sub_dir: bool\n",
        "    :rtype: None\n",
        "    \"\"\"\n",
        "    for dir in os.listdir(dir_path):\n",
        "      sub_dir = os.path.join(dir_path, dir)\n",
        "      \n",
        "      if os.path.isfile(sub_dir): # if it's a file then clean the file  \n",
        "        if only_sub_dir: # no subfiles allowed for example in the pixel-art-tagged \n",
        "          os.remove(sub_dir)\n",
        "          print(f\"[Removing] {sub_dir}\")\n",
        "          continue \n",
        "        clean_file(sub_dir)\n",
        "        continue\n",
        "\n",
        "      if len(os.listdir(sub_dir)) == 0: # Empty folder, delte it \n",
        "        shutil.rmtree(sub_dir)\n",
        "        #os.system(f'rm -r {sub_dir}') \n",
        "        print(f'[Removing] {sub_dir}')\n",
        "        continue\n",
        "\n",
        "      if os.path.isdir(sub_dir) and only_sub_dir: # move to the sub-directory and clean it.\n",
        "        clean_directory(sub_dir)\n",
        "      else:\n",
        "        shutil.rmtree(sub_dir)  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5lPgT09PHX6"
      },
      "source": [
        "### Unzipping and cleaning for the tagged images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ld05ynBPQxo",
        "outputId": "31b2c247-029b-476a-cbf5-e717427574b5"
      },
      "outputs": [],
      "source": [
        "if is_archive(tagged_dataset_path):\n",
        "  tagged_images_folder = unzip_folder(tagged_dataset_path)\n",
        "  clean_directory(tagged_images_folder, only_sub_dir=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeIimd1gcuvY"
      },
      "source": [
        "### set required variables by the utility\n",
        "\n",
        "Initialize the required parameters needed by the dataset preprocessor utility and they are described as follows: \n",
        "\n",
        "* `input_folder` _[str]_ -  path to the directory containing sub-folders of each tag.\n",
        "* `output_folder` _[str]_ - path to the directory where to save the files into it.\n",
        "\n",
        "* `clip_model` _[str]_ - CLIP model to be used\n",
        "\n",
        "* `pretrained` _[str]_ - the pre-trained model to be used for CLIP\n",
        "* `batch_size` _[int]_ -  number of images to process at a time\n",
        "* `num_threads` _[int]_ - the number to be used in this process\n",
        "\n",
        "* `device` _[str]_ -  the device to be used in computing the CLIP embeddings, if `None` is provided then `cuda` will be used if available\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sFReXmwmcuvY"
      },
      "outputs": [],
      "source": [
        "dataset_path = tagged_images_folder\n",
        "output_folder = \"output\"\n",
        "clip_model = \"ViT-B-32\"\n",
        "pretrained = 'openai'\n",
        "batch_size = 32\n",
        "num_threads = 4\n",
        "device = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D691ZEnrcuvZ"
      },
      "source": [
        "### Run the Preprocessor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZPLvQ1EcuvZ",
        "outputId": "314a1c84-f94f-4513-ad23-98f94cf03c3a"
      },
      "outputs": [],
      "source": [
        "ImageDatasetProcessor.process_dataset(\n",
        "    dataset_path, \n",
        "    output_folder, \n",
        "    clip_model, \n",
        "    pretrained,\n",
        "    batch_size, \n",
        "    num_threads, \n",
        "    device\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ld8q1avSOdOP"
      },
      "source": [
        "### Train script variables\n",
        "\n",
        "\n",
        "* `metadata_json` _[string]_ - _[required]_ - The path to the metadata json file. \n",
        "* `tag_to_hash_json` _[string]_ - _[required]_ - The path to tag-to-hash json file. \n",
        "\n",
        "* `output` _[string]_ - _[optional]_ - The path to the output directory.\n",
        "* `test_per` _[float]_ - _[optional]_ - The percentage of the test images from the dataset, default = 0.1 \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fYigdGm_PHLm"
      },
      "outputs": [],
      "source": [
        "metadata_json = './output/input-metadata.json' \n",
        "tag_to_hash_json = './output/input-tag-to-image-hash-list.json'\n",
        "output_dir = './output'\n",
        "test_per = 0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GLoJWDJS1vm"
      },
      "source": [
        "### Run training script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjWOmPV1-3Zz",
        "outputId": "8f7599aa-1beb-4535-a2c4-21021cc1692e"
      },
      "outputs": [],
      "source": [
        "train_main(\n",
        "    metadata_json = metadata_json,\n",
        "    tag_to_hash_json = tag_to_hash_json,\n",
        "    output_dir = output_dir,\n",
        "    test_per = test_per\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtcQghX_S5YD"
      },
      "source": [
        "### Classification script variables\n",
        "\n",
        "* `directory` _[string]_ - _[required]_ - The path to the images' folder or images' .zip file. \n",
        "* `metadata_json` _[string]_ - _[required]_ - The path to the metadata json file for CLIP embeddings. \n",
        "* `output` _[string]_ - _[optional]_ - The path to the output directory for the inference results. \n",
        "* `model` _[string]_ - _[optional]_ - The path to the models' .pkl files directory or single .pkl file model.\n",
        "* `output_bins` _[int]_ - _[optional]_ -  The number of bins of the results for each model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWUepQsbWrT1"
      },
      "source": [
        "#### Classification fot other-validation folder (pre-computed CLIP embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wvcwd-mVQ_A8"
      },
      "outputs": [],
      "source": [
        "folder_path    = os.path.join(tagged_images_folder,\"other-validation\")\n",
        "output_dir     = \"./classification_other_validation\"\n",
        "json_file_path =  \"/content/output/input-metadata.json\"\n",
        "bins_number    = 10\n",
        "model_path     = \"./output/models\" "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hj7cPMRThnLC",
        "outputId": "df64e9ed-cc42-4e3f-f741-a12bfb50da60"
      },
      "outputs": [],
      "source": [
        "classify_main(\n",
        "        folder_path    = folder_path , \n",
        "        output_dir     = output_dir, \n",
        "        json_file_path = json_file_path, \n",
        "        bins_number    = bins_number , \n",
        "        model_path     = model_path, \n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nsNclcPX49V"
      },
      "source": [
        "#### Classifction for .zip file full of images (takes more time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDRrhjs4wTkw"
      },
      "outputs": [],
      "source": [
        "folder_path    = other_dataset_path\n",
        "output_dir     = \"./classification_other_folder\"\n",
        "json_file_path =  \"/content/output/input-metadata.json\"\n",
        "bins_number    = 10\n",
        "model_path     = \"./output/models\" "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AwyCnb0dXSGc"
      },
      "outputs": [],
      "source": [
        "classify_main(\n",
        "        folder_path    = folder_path , \n",
        "        output_dir     = output_dir, \n",
        "        json_file_path = json_file_path, \n",
        "        bins_number    = bins_number , \n",
        "        model_path     = model_path, \n",
        "        )"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.0 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "b8e93f441cdd34ec3e1aab4168a471abe1992cd5890142962f4e98cab1379cc7"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
