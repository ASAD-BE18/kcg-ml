{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhEVe0JFcuu_"
      },
      "source": [
        "# Mount Google Drive\n",
        "\n",
        "Mount your google drive to be used for storing the storing the dataset into it (optional to save datasets on mega to it) and also for saving the results in the drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ONxui2_ccuvK",
        "outputId": "464ce773-3a3e-4ff8-8625-4c08c0008e4b"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount._DEBUG = False\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fW5yYaLVcuvM"
      },
      "source": [
        "# Move Dataset from Mega to Drive\n",
        "\n",
        "Move the file with the provided link (Provide your username and password if it's not a public link) to your google drive already mounted before in the previous step. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dK4RNS2lcuvN"
      },
      "source": [
        "### Install Mega"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6nd_7Rg6cuvO"
      },
      "outputs": [],
      "source": [
        "import sys, os, urllib.request\n",
        "import time\n",
        "import subprocess\n",
        "import contextlib\n",
        "from IPython.display import clear_output\n",
        "from functools import wraps\n",
        "import errno\n",
        "import os\n",
        "import signal\n",
        "import subprocess\n",
        "import shlex\n",
        "import glob\n",
        "\n",
        "HOME = os.path.expanduser(\"~\")\n",
        "if not os.path.exists(f\"{HOME}/.ipython/ocr.py\"):\n",
        "    hCode = \"https://raw.githubusercontent.com/biplobsd/\" \\\n",
        "                \"OneClickRun/master/res/ocr.py\"\n",
        "    urllib.request.urlretrieve(hCode, f\"{HOME}/.ipython/ocr.py\")\n",
        "\n",
        "from ocr import (\n",
        "    runSh,\n",
        "    loadingAn,\n",
        ")\n",
        "\n",
        "if not os.path.exists(\"/usr/bin/mega-cmd\"):\n",
        "    loadingAn()\n",
        "    print(\"Installing MEGA ...\")\n",
        "    runSh('sudo apt-get -y update')\n",
        "    runSh('sudo apt-get -y install libmms0 libc-ares2 libc6 libcrypto++6 libgcc1 libmediainfo0v5 libpcre3 libpcrecpp0v5 libssl1.1 libstdc++6 libzen0v5 zlib1g apt-transport-https')\n",
        "    runSh('sudo curl -sL -o /var/cache/apt/archives/MEGAcmd.deb https://mega.nz/linux/MEGAsync/Debian_9.0/amd64/megacmd-Debian_9.0_amd64.deb', output=True)\n",
        "    runSh('sudo dpkg -i /var/cache/apt/archives/MEGAcmd.deb', output=True)\n",
        "    print(\"MEGA is installed.\")\n",
        "    clear_output()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Pi_Rk09cuvQ"
      },
      "source": [
        "### provide URL and Mega ID\n",
        "\n",
        "Add the URL to fetch from Mega, and the username and password for those files if it wasn't a public URL and to use the pro mega quota if you have a pro account."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9jy2wxACPmn"
      },
      "outputs": [],
      "source": [
        "#It's optional to provide the MEGA username and password, it's used for giving more download quota if you have a MEGA pro account. \n",
        "MEGA_USERNAME = \"\"  #optional \n",
        "MEGA_PASSWORD = \"\"  #optional \n",
        "\n",
        "TAGGED_DATASET_URL = \"https://mega.nz/file/kc40hbgL#WH0rLmRwkJodzD4yazWWAlnmp_IJQiLG0jx2hhDJLhY\"\n",
        "OTHER_DATASET_URL  = \"https://mega.nz/file/MNo0ABSB#8rDqevxdQmtaNFKjQ3RS9v2pF-jL8xzmM9LLxIfAhG0\"\n",
        "OUTPUT_PATH = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjz0-BUHcuvS",
        "outputId": "d59e26fd-0be2-4da3-e546-458c74a1a20d"
      },
      "outputs": [],
      "source": [
        "# Unix, Windows and old Macintosh end-of-line\n",
        "newlines = ['\\n', '\\r\\n', '\\r']\n",
        "\n",
        "def latest_file(folder):\n",
        "  list_of_files = glob.glob(f'{folder}/*') # * means all \n",
        "  latest_file = max(list_of_files, key=os.path.getctime)\n",
        "  return latest_file\n",
        "\n",
        "def unbuffered(proc, stream='stdout'):\n",
        "    stream = getattr(proc, stream)\n",
        "    with contextlib.closing(stream):\n",
        "        while True:\n",
        "            out = []\n",
        "            last = stream.read(1)\n",
        "            # Don't loop forever\n",
        "            if last == '' and proc.poll() is not None:\n",
        "                break\n",
        "            while last not in newlines:\n",
        "                # Don't loop forever\n",
        "                if last == '' and proc.poll() is not None:\n",
        "                    break\n",
        "                out.append(last)\n",
        "                last = stream.read(1)\n",
        "            out = ''.join(out)\n",
        "            yield out\n",
        "\n",
        "\n",
        "def transfer(url):\n",
        "    import codecs\n",
        "    decoder = codecs.getincrementaldecoder(\"UTF-8\")()\n",
        "    cmd = [\"mega-get\", url, OUTPUT_PATH]\n",
        "    proc = subprocess.Popen(\n",
        "        cmd,\n",
        "        stdout=subprocess.PIPE,\n",
        "        stderr=subprocess.STDOUT,\n",
        "        # Make all end-of-lines '\\n'\n",
        "        universal_newlines=True,\n",
        "    )\n",
        "    for line in unbuffered(proc):\n",
        "        print(line)\n",
        "        \n",
        "if not OUTPUT_PATH:\n",
        "  os.makedirs(\"downloads\", exist_ok=True)\n",
        "  OUTPUT_PATH = \"downloads\"\n",
        "\n",
        "\n",
        "class TimeoutError(Exception):\n",
        "    pass\n",
        "\n",
        "def timeout(seconds=10, error_message=os.strerror(errno.ETIME)):\n",
        "    def decorator(func):\n",
        "        def _handle_timeout(signum, frame):\n",
        "            raise TimeoutError(error_message)\n",
        "\n",
        "        def wrapper(*args, **kwargs):\n",
        "            signal.signal(signal.SIGALRM, _handle_timeout)\n",
        "            signal.alarm(seconds)\n",
        "            try:\n",
        "                result = func(*args, **kwargs)\n",
        "            finally:\n",
        "                signal.alarm(0)\n",
        "            return result\n",
        "\n",
        "        return wraps(func)(wrapper)\n",
        "\n",
        "    return decorator\n",
        "\n",
        "\n",
        "@timeout(10)\n",
        "def runShT(args):\n",
        "    return runSh(args, output=True)\n",
        "\n",
        "def login(): \n",
        "    runShT(f\"mega-login {MEGA_USERNAME} {MEGA_PASSWORD}\")\n",
        "\n",
        "#if the username and password provided then login to MEGA. \n",
        "if MEGA_USERNAME != \"\" and MEGA_PASSWORD != \"\":\n",
        "    try:\n",
        "        login()\n",
        "    except TimeoutError:\n",
        "        runSh('mega-whoami', output=True)\n",
        "else:\n",
        "    print(\"Please Input your Mega IDs.\")\n",
        "\n",
        "transfer(TAGGED_DATASET_URL)\n",
        "tagged_dataset_path = latest_file('./downloads')\n",
        "transfer(OTHER_DATASET_URL)\n",
        "other_dataset_path = latest_file('./downloads')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFI20pLVcuvU"
      },
      "source": [
        "# Fetch & Clone Repo\n",
        "\n",
        "Clone the `image-tagging-tools` repo as it has all the required utils and codes from preprocessing the image datasets to training the models and using them to classify the images. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6n6QEB49cuvV",
        "outputId": "694d6054-03f6-4fda-9836-cc6dad61811b"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/kk-digital/image-tagging-tools.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hYcmeZicuvW"
      },
      "source": [
        "# Preprocess the dataset images (Stage 1)\n",
        "\n",
        "Use the `image-dataset-processor` utils from the previously cloned repo to process a directory of images (paths to directory of images or an archived dataset), and computes the images metadata along with its CLIP embeddings and writes the result into a JSON file into `output_folder`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guwuN0UncuvW"
      },
      "source": [
        "### Install requirements "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86S-QyvSnwG8"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%pip install ascii_graph open_clip_torch patool fire"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiqXMqOEcuvX"
      },
      "source": [
        "### Import the module/Utility "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "usaH8e-fcuvY"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.insert(0, '../image-tagging-tools/stage1/')\n",
        "sys.path.insert(0, '../image-tagging-tools/stage2/')\n",
        "sys.path.insert(0, '../image-tagging-tools/stage3/')\n",
        "sys.path.insert(0, '../image-tagging-tools/stage4/')\n",
        "from ImageDatasetProcessor import ImageDatasetProcessor\n",
        "from classify import main as classify_main\n",
        "from classify_zip import main as classify_main_zip\n",
        "from train import main as train_main\n",
        "from classify_helper_functions import *\n",
        "from classify_zip_helper_functions import *\n",
        "import patoolib\n",
        "import shutil\n",
        "\n",
        "\n",
        "def is_archive(path: str) -> bool:\n",
        "    \"\"\"method to check if a given path is an archive.\n",
        "    :param path: The file path to check. \n",
        "    :type path: str\n",
        "    :returns: `True` if the given path is a path of an archived file. \n",
        "    :rtype: bool\n",
        "    \"\"\"\n",
        "    try: \n",
        "        patoolib.get_archive_format(path)\n",
        "        return True \n",
        "    except Exception: \n",
        "        return False \n",
        "\n",
        "def unzip_folder(folder_path :str):\n",
        "    \"\"\"takes an archived file path and unzip it.\n",
        "    :param folder_path: path to the archived file.\n",
        "    :type folder_path: str\n",
        "    :returns: path of the new exracted folder \n",
        "    :rtype: str\n",
        "    \"\"\"\n",
        "    dir_path  = os.path.dirname(folder_path)\n",
        "    file_name = os.path.basename(folder_path).split('.zip')[0]\n",
        "    os.makedirs(dir_path , exist_ok=True)\n",
        "    \n",
        "    print(\"[INFO] Extracting the archived file...\")\n",
        "    patoolib.extract_archive(folder_path, outdir=dir_path)\n",
        "    print(\"[INFO] Extraction completed.\")\n",
        "    \n",
        "    return latest_file(dir_path)\n",
        "\n",
        "\n",
        "def clean_directory(dir_path: str, only_sub_dir: bool = False):\n",
        "    \n",
        "    for dir in os.listdir(dir_path):\n",
        "        sub_dir = os.path.join(dir_path, dir)\n",
        "        \n",
        "        if os.path.isfile(sub_dir): # It is a file. Check and break if it is outside of tag folder\n",
        "            if only_sub_dir: \n",
        "                '''RV: Deletion codes are removed since it should not delete anything. Prompt error instead'''\n",
        "                raise Exception (f'[ERROR: Input dataset contains file outside of tag folder: {sub_dir}]')\n",
        "\n",
        "            ImageDatasetLoader.check_file(sub_dir)\n",
        "            continue\n",
        "\n",
        "        if len(os.listdir(sub_dir)) == 0: # Empty folder\n",
        "            '''RV: Deletion codes are removed since it should not delete anything. Prompt error instead'''\n",
        "            raise Exception (f'[ERROR]: Input dataset contains empty folder: {sub_dir}]')\n",
        "\n",
        "        if os.path.isdir(sub_dir) and only_sub_dir: # move to the sub-directory and clean it.\n",
        "            ImageDatasetLoader.clean_directory(sub_dir)\n",
        "        else:\n",
        "            '''RV: Deletion codes are removed since it should not delete anything. Prompt error instead'''\n",
        "            raise Exception ('[ERROR]: Dataset format is possible invalid...]')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5lPgT09PHX6"
      },
      "source": [
        "### Unzipping and cleaning for the tagged images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ld05ynBPQxo",
        "outputId": "31b2c247-029b-476a-cbf5-e717427574b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Extracting the archived file...\n",
            "patool: Extracting ./dataset/subset_3.zip ...\n",
            "patool: running \"C:\\Users\\Reza Vilera\\scoop\\shims\\7z.EXE\" x -o./dataset -- ./dataset/subset_3.zip\n",
            "patool: ... ./dataset/subset_3.zip extracted to `./dataset'.\n",
            "[INFO] Extraction completed.\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'latest_file' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn [36], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m tagged_dataset_path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m./dataset/subset_3.zip\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[39mif\u001b[39;00m is_archive(tagged_dataset_path):\n\u001b[1;32m----> 4\u001b[0m   tagged_images_folder \u001b[39m=\u001b[39m unzip_folder(tagged_dataset_path)\n\u001b[0;32m      5\u001b[0m   clean_directory(tagged_images_folder, only_sub_dir\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
            "Cell \u001b[1;32mIn [29], line 44\u001b[0m, in \u001b[0;36munzip_folder\u001b[1;34m(folder_path)\u001b[0m\n\u001b[0;32m     41\u001b[0m patoolib\u001b[39m.\u001b[39mextract_archive(folder_path, outdir\u001b[39m=\u001b[39mdir_path)\n\u001b[0;32m     42\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m[INFO] Extraction completed.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 44\u001b[0m \u001b[39mreturn\u001b[39;00m latest_file(dir_path)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'latest_file' is not defined"
          ]
        }
      ],
      "source": [
        "if is_archive(tagged_dataset_path):\n",
        "  tagged_images_folder = unzip_folder(tagged_dataset_path)\n",
        "  clean_directory(tagged_images_folder, only_sub_dir=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeIimd1gcuvY"
      },
      "source": [
        "### set required variables by the utility\n",
        "\n",
        "Initialize the required parameters needed by the dataset preprocessor utility and they are described as follows: \n",
        "\n",
        "* `input_folder` _[str]_ -  path to the directory containing sub-folders of each tag.\n",
        "* `output_folder` _[str]_ - path to the directory where to save the files into it.\n",
        "\n",
        "* `clip_model` _[str]_ - CLIP model to be used\n",
        "\n",
        "* `pretrained` _[str]_ - the pre-trained model to be used for CLIP\n",
        "* `batch_size` _[int]_ -  number of images to process at a time\n",
        "* `num_threads` _[int]_ - the number to be used in this process\n",
        "\n",
        "* `device` _[str]_ -  the device to be used in computing the CLIP embeddings, if `None` is provided then `cuda` will be used if available\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "sFReXmwmcuvY"
      },
      "outputs": [],
      "source": [
        "dataset_path = tagged_images_folder\n",
        "output_folder = \"./output\"\n",
        "tagged_dataset = True\n",
        "clip_model = \"ViT-B-32\"\n",
        "pretrained = 'openai'\n",
        "batch_size = 32\n",
        "num_threads = 4\n",
        "device = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D691ZEnrcuvZ"
      },
      "source": [
        "### Run the Preprocessor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZPLvQ1EcuvZ",
        "outputId": "314a1c84-f94f-4513-ad23-98f94cf03c3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Writing to database table in ./output//stage1.db\n"
          ]
        }
      ],
      "source": [
        "ImageDatasetProcessor.process_dataset(\n",
        "    dataset_path, \n",
        "    output_folder,\n",
        "    tagged_dataset, \n",
        "    clip_model, \n",
        "    pretrained,\n",
        "    batch_size, \n",
        "    num_threads, \n",
        "    device\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ld8q1avSOdOP"
      },
      "source": [
        "### Train script variables\n",
        "\n",
        "\n",
        "* `metadata_json` _[string]_ - _[required]_ - The path to the metadata json file. \n",
        "* `tag_to_hash_json` _[string]_ - _[required]_ - The path to tag-to-hash json file. \n",
        "\n",
        "* `output` _[string]_ - _[optional]_ - The path to the output directory.\n",
        "* `test_per` _[float]_ - _[optional]_ - The percentage of the test images from the dataset, default = 0.1 \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "fYigdGm_PHLm"
      },
      "outputs": [],
      "source": [
        "metadata_json = './image-tagging-tools/output/input-metadta.json' \n",
        "tag_to_hash_json = './image-tagging-tools/output/input-tag-to-image-hash-list.json'\n",
        "output_dir = './image-tagging-tools/output'\n",
        "test_per = 0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GLoJWDJS1vm"
      },
      "source": [
        "### Run training script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjWOmPV1-3Zz",
        "outputId": "8f7599aa-1beb-4535-a2c4-21021cc1692e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Finished.\n"
          ]
        }
      ],
      "source": [
        "train_main(\n",
        "    metadata_json = metadata_json,\n",
        "    tag_to_hash_json = tag_to_hash_json,\n",
        "    output_dir = output_dir,\n",
        "    test_per = test_per\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Listing all the models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "1)  ovr-logistic-regression\n",
            "\t \t \tnot-pixel-art-digital\n",
            "\t \t \tnot-pixel-art-real-photo\n",
            "\t \t \tnot-pixel-art\n",
            "\t \t \tpos-character-portrait\n",
            "\t \t \tpos-character\n",
            "\t \t \tpos-color-pallete\n",
            "\t \t \tpos-environment-architecture\n",
            "\t \t \tpos-environment-natural\n",
            "\t \t \tpos-environment-space\n",
            "\t \t \tpos-environment\n",
            "\t \t \tpos-environmental-architecture\n",
            "\t \t \tpos-environmental-natural\n",
            "\t \t \tpos-environmental-space\n",
            "\t \t \tpos-machines\n",
            "\t \t \tpos-pixel-art-architecture\n",
            "\t \t \tpos-pixel-art-bad\n",
            "\t \t \tpos-pixel-art-best\n",
            "\t \t \tpos-pixel-art-character\n",
            "\t \t \tpos-pixel-art-environmental\n",
            "\t \t \tpos-pixel-art-scifi\n",
            "\t \t \tpos-pixel-art-space\n",
            "\t \t \tpos-pixel-art-texture\n",
            "\t \t \tpos-pixel-art-tutorial\n",
            "\t \t \tpos-pixel-art\n",
            "\t \t \tpos-scifi\n",
            "\t \t \tpos-sprite-sheet\n",
            "\t \t \tpos-style-comic\n",
            "\t \t \tpos-style-synth-wave\n",
            "\t \t \tpos-style-tile-based\n",
            "\t \t \tpos-video-game-3d\n",
            "\t \t \tpos-video-game-flat\n",
            "\t \t \tpos-video-game-isometric\n",
            "\t \t \tpos-video-game-map\n",
            "\t \t \tpos-video-game-side-scrolling\n",
            "\t \t \tpos-video-game-style-miltech-kag\n",
            "\t \t \tpos-video-game-top-down\n",
            "\t \t \tpos-video-game-top-perspective\n",
            "\t \t \tpos-video-game\n",
            "\t \t \tpos-waifus\n",
            "\n",
            "2)  ovr-svm\n",
            "\t \t \tnot-pixel-art-digital\n",
            "\t \t \tnot-pixel-art-real-photo\n",
            "\t \t \tnot-pixel-art\n",
            "\t \t \tpos-character-portrait\n",
            "\t \t \tpos-character\n",
            "\t \t \tpos-color-pallete\n",
            "\t \t \tpos-environment-architecture\n",
            "\t \t \tpos-environment-natural\n",
            "\t \t \tpos-environment-space\n",
            "\t \t \tpos-environment\n",
            "\t \t \tpos-environmental-architecture\n",
            "\t \t \tpos-environmental-natural\n",
            "\t \t \tpos-environmental-space\n",
            "\t \t \tpos-machines\n",
            "\t \t \tpos-pixel-art-architecture\n",
            "\t \t \tpos-pixel-art-bad\n",
            "\t \t \tpos-pixel-art-best\n",
            "\t \t \tpos-pixel-art-character\n",
            "\t \t \tpos-pixel-art-environmental\n",
            "\t \t \tpos-pixel-art-scifi\n",
            "\t \t \tpos-pixel-art-space\n",
            "\t \t \tpos-pixel-art-texture\n",
            "\t \t \tpos-pixel-art-tutorial\n",
            "\t \t \tpos-pixel-art\n",
            "\t \t \tpos-scifi\n",
            "\t \t \tpos-sprite-sheet\n",
            "\t \t \tpos-style-comic\n",
            "\t \t \tpos-style-synth-wave\n",
            "\t \t \tpos-style-tile-based\n",
            "\t \t \tpos-video-game-3d\n",
            "\t \t \tpos-video-game-flat\n",
            "\t \t \tpos-video-game-isometric\n",
            "\t \t \tpos-video-game-map\n",
            "\t \t \tpos-video-game-side-scrolling\n",
            "\t \t \tpos-video-game-style-miltech-kag\n",
            "\t \t \tpos-video-game-top-down\n",
            "\t \t \tpos-video-game-top-perspective\n",
            "\t \t \tpos-video-game\n",
            "\t \t \tpos-waifus\n"
          ]
        }
      ],
      "source": [
        "list_models('./output/models') # listing all the models we have for classification. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtcQghX_S5YD"
      },
      "source": [
        "### Classification script variables\n",
        "\n",
        "* `directory` _[string]_ - _[required]_ - The path to the images' folder or images' .zip file. \n",
        "* `metadata_json` _[string]_ - _[required]_ - The path to the metadata json file for CLIP embeddings. \n",
        "* `output` _[string]_ - _[optional]_ - The path to the output directory for the inference results. \n",
        "* `model` _[string]_ - _[optional]_ - The path to the models' .pkl files directory or single .pkl file model.\n",
        "* `output_bins` _[int]_ - _[optional]_ -  The number of bins of the results for each model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWUepQsbWrT1"
      },
      "source": [
        "#### Classification fot other-validation folder (pre-computed CLIP embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "wvcwd-mVQ_A8"
      },
      "outputs": [],
      "source": [
        "folder_path    = os.path.join(tagged_images_folder,\"other-validation\")\n",
        "output_dir     = \"./classification_other_validation\"\n",
        "json_file_path =  \"./output/input-metadata.json\"\n",
        "bins_number    = 10\n",
        "model_path     = \"./output/models\" "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hj7cPMRThnLC",
        "outputId": "df64e9ed-cc42-4e3f-f741-a12bfb50da60"
      },
      "outputs": [],
      "source": [
        "classify_main(\n",
        "        folder_path    = folder_path , \n",
        "        output_dir     = output_dir, \n",
        "        json_file_path = json_file_path, \n",
        "        bins_number    = bins_number , \n",
        "        model_path     = model_path, \n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Single Image -- Single Model Classification Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Image file to be classified \n",
        "folder_path    = \"/content/downloads/pixel-art-tagged-v2/other-validation/https___i.pinimg.com_originals_00_79_2c_00792c0d83d415a707bdacee51646d9e.png\"\n",
        "output_dir     = \".output/classification_single_image_single_model\"\n",
        "json_file_path = \".output/input-metadata.json\"\n",
        "bins_number    = 10\n",
        "model_path     = \"./output/models/model-ovr-logistic-regression-tag-not-pixel-art-digital.pkl\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Output folder ./classification_other_validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00,  6.15it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Writing to database table in ./output//stage3.db\n",
            "[INFO] Finished.\n"
          ]
        }
      ],
      "source": [
        "classify_main(\n",
        "        folder_path    = folder_path , \n",
        "        output_dir     = output_dir, \n",
        "        json_file_path = json_file_path, \n",
        "        bins_number    = bins_number, \n",
        "        model_path     = model_path, \n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Single image -- All the models classification example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Image file to be classified \n",
        "folder_path    = \"/content/downloads/pixel-art-tagged-v2/other-validation/https___i.pinimg.com_originals_00_79_2c_00792c0d83d415a707bdacee51646d9e.png\"\n",
        "output_dir     = \"./output/classification_single_image_all_models\"\n",
        "json_file_path = \"./output/input-metadata.json\"\n",
        "bins_number    = 10\n",
        "model_path = \"./output/models\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Output folder ./classification_other_validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00,  4.84it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Writing to database table in ./output//stage3.db\n",
            "[INFO] Finished.\n"
          ]
        }
      ],
      "source": [
        "classify_main(\n",
        "        folder_path    = folder_path , \n",
        "        output_dir     = output_dir, \n",
        "        json_file_path = json_file_path, \n",
        "        bins_number    = bins_number, \n",
        "        model_path     = model_path, \n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model and Tag name Classification for Single Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "TAG_NAME   = 'not-pixel-art' # tag which you want to classify.\n",
        "MODEL_TYPE = 'ovr-logistic-regression' # model type you want to use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Image file to be classified \n",
        "folder_path    = \"/content/downloads/pixel-art-tagged-v2/other-validation/https___i.pinimg.com_originals_00_79_2c_00792c0d83d415a707bdacee51646d9e.png\"\n",
        "output_dir     = \".output/classification_single_image_custom_model\"\n",
        "json_file_path = \"./output/input-metadata.json\"\n",
        "bins_number    = 10\n",
        "model_path = generate_model_path(\n",
        "                                  './output/models',\n",
        "                                  model_type= MODEL_TYPE,\n",
        "                                  tag_name= TAG_NAME\n",
        "                                 )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[WinError 3] The system cannot find the path specified: '/content/downloads/pixel-art-tagged-v2/other-validation/https___i.pinimg.com_originals_00_79_2c_00792c0d83d415a707bdacee51646d9e.png'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn [35], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m classify_main(\n\u001b[0;32m      2\u001b[0m         folder_path    \u001b[39m=\u001b[39;49m folder_path, \n\u001b[0;32m      3\u001b[0m         output_dir     \u001b[39m=\u001b[39;49m output_dir, \n\u001b[0;32m      4\u001b[0m         json_file_path \u001b[39m=\u001b[39;49m json_file_path, \n\u001b[0;32m      5\u001b[0m         bins_number    \u001b[39m=\u001b[39;49m bins_number, \n\u001b[0;32m      6\u001b[0m         model_path     \u001b[39m=\u001b[39;49m model_path, \n\u001b[0;32m      7\u001b[0m         )\n",
            "File \u001b[1;32me:\\vscode\\image-tagging-tools\\stage3\\classify.py:38\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(folder_path, output_dir, json_file_path, bins_number, model_path)\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[39mreturn\u001b[39;00m \n\u001b[0;32m     36\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misfile(folder_path):\n\u001b[0;32m     37\u001b[0m     \u001b[39m# Check for empty dirs\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m     empty_dirs_check(folder_path)\n\u001b[0;32m     39\u001b[0m     \u001b[39m# Placeholder for dataset file names\u001b[39;00m\n\u001b[0;32m     40\u001b[0m     img_files_list \u001b[39m=\u001b[39m []\n",
            "File \u001b[1;32me:\\vscode\\image-tagging-tools\\stage3\\classify_helper_functions.py:484\u001b[0m, in \u001b[0;36mempty_dirs_check\u001b[1;34m(dir_path)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mempty_dirs_check\u001b[39m(dir_path : \u001b[39mstr\u001b[39m):\n\u001b[0;32m    483\u001b[0m       \u001b[39m\"\"\" Checking for empty directory\"\"\"\u001b[39;00m\n\u001b[1;32m--> 484\u001b[0m       \u001b[39mfor\u001b[39;00m \u001b[39mdir\u001b[39m \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39;49mlistdir(dir_path):\n\u001b[0;32m    485\u001b[0m         sub_dir \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(dir_path, \u001b[39mdir\u001b[39m)\n\u001b[0;32m    487\u001b[0m         \u001b[39m# Check for directory only\u001b[39;00m\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: '/content/downloads/pixel-art-tagged-v2/other-validation/https___i.pinimg.com_originals_00_79_2c_00792c0d83d415a707bdacee51646d9e.png'"
          ]
        }
      ],
      "source": [
        "classify_main(\n",
        "        folder_path    = folder_path, \n",
        "        output_dir     = output_dir, \n",
        "        json_file_path = json_file_path, \n",
        "        bins_number    = bins_number, \n",
        "        model_path     = model_path, \n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nsNclcPX49V"
      },
      "source": [
        "#### Classifcation for .zip file full of images (takes more time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "NDRrhjs4wTkw"
      },
      "outputs": [],
      "source": [
        "folder_path    = 'dataset/subset3_.zip'\n",
        "output_dir     = './output/tagging_output_from_zip'\n",
        "json_file_path =  \"./output/input-metadata.json\"\n",
        "bins_number    = 5\n",
        "model_path     = \"./output/models\" "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "AwyCnb0dXSGc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Output folder ./output/tagging_output_from_zip\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/1 [00:00<?, ?it/s]\n",
            "0it [00:00, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Working on ZIP archive: dataset/subset3_.zip\n",
            " Processing: subset3/not-pixel-art-real-photo/10.jpg\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "1it [00:00,  2.31it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Processing: subset3/not-pixel-art-real-photo/11.jpg\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "2it [00:00,  2.73it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Processing: subset3/not-pixel-art-real-photo/12.jpg\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "3it [00:01,  3.19it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Processing: subset3/not-pixel-art-real-photo/13.jpg\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "4it [00:01,  3.28it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Processing: subset3/not-pixel-art-real-photo/14.jpg\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "5it [00:01,  3.18it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Processing: subset3/not-pixel-art-real-photo/15.jpg\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "6it [00:02,  2.39it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Processing: subset3/not-pixel-art-real-photo/16.jpg\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "7it [00:02,  2.50it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Processing: subset3/not-pixel-art-real-photo/9.jpg\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "8it [00:03,  1.67it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Processing: subset3/not-pixel-art/1.jpg\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "9it [00:04,  1.83it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Processing: subset3/not-pixel-art/2.jpg\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "10it [00:04,  2.07it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Processing: subset3/not-pixel-art/3.jpg\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "11it [00:06,  1.03it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Processing: subset3/not-pixel-art/4.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "12it [00:07,  1.01s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Processing: subset3/not-pixel-art/5.jpg\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "13it [00:08,  1.04s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Processing: subset3/not-pixel-art/6.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "14it [00:09,  1.06it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Processing: subset3/not-pixel-art/7.jpg\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "15it [00:10,  1.07it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Processing: subset3/not-pixel-art/8.jpg\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "16it [00:10,  1.26it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Working on ZIP archive: dataset/subset3_.zip/subset3/subset3_subzip1.zip\n",
            " Processing: dataset/subset3_.zip/10_zip.jpg\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "17it [00:11,  1.39it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Processing: dataset/subset3_.zip/11_zip.jpg\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "18it [00:11,  1.64it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Processing: dataset/subset3_.zip/12_zip.jpg\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "19it [00:11,  1.99it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Processing: dataset/subset3_.zip/9_zip.jpg\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "20it [00:12,  1.54it/s]\n",
            "100%|██████████| 1/1 [00:12<00:00, 12.97s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "./output//stage4.db\n",
            "[INFO] Writing to database table in ./output//stage4.db\n",
            "[INFO] Finished.\n"
          ]
        }
      ],
      "source": [
        "classify_main_zip(\n",
        "        folder_path    = folder_path , \n",
        "        output_dir     = output_dir, \n",
        "        json_file_path = json_file_path, \n",
        "        bins_number    = bins_number , \n",
        "        model_path     = model_path, \n",
        "        )"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "c518711542db1b7752d0b1005bb6b1db13084b2ce60111ae8895922158ddc3d4"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
