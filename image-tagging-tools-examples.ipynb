{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhEVe0JFcuu_"
      },
      "source": [
        "# Mount Google Drive\n",
        "\n",
        "Mount your google drive to be used for storing the storing the dataset into it (optional to save datasets on mega to it) and also for saving the results in the drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ONxui2_ccuvK",
        "outputId": "464ce773-3a3e-4ff8-8625-4c08c0008e4b"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount._DEBUG = False\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fW5yYaLVcuvM"
      },
      "source": [
        "# Move Dataset from Mega to Drive\n",
        "\n",
        "Move the file with the provided link (Provide your username and password if it's not a public link) to your google drive already mounted before in the previous step. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dK4RNS2lcuvN"
      },
      "source": [
        "### Install Mega"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6nd_7Rg6cuvO"
      },
      "outputs": [],
      "source": [
        "import sys, os, urllib.request\n",
        "import time\n",
        "import subprocess\n",
        "import contextlib\n",
        "from IPython.display import clear_output\n",
        "from functools import wraps\n",
        "import errno\n",
        "import os\n",
        "import signal\n",
        "import subprocess\n",
        "import shlex\n",
        "import glob\n",
        "\n",
        "HOME = os.path.expanduser(\"~\")\n",
        "if not os.path.exists(f\"{HOME}/.ipython/ocr.py\"):\n",
        "    hCode = \"https://raw.githubusercontent.com/biplobsd/\" \\\n",
        "                \"OneClickRun/master/res/ocr.py\"\n",
        "    urllib.request.urlretrieve(hCode, f\"{HOME}/.ipython/ocr.py\")\n",
        "\n",
        "from ocr import (\n",
        "    runSh,\n",
        "    loadingAn,\n",
        ")\n",
        "\n",
        "if not os.path.exists(\"/usr/bin/mega-cmd\"):\n",
        "    loadingAn()\n",
        "    print(\"Installing MEGA ...\")\n",
        "    runSh('sudo apt-get -y update')\n",
        "    runSh('sudo apt-get -y install libmms0 libc-ares2 libc6 libcrypto++6 libgcc1 libmediainfo0v5 libpcre3 libpcrecpp0v5 libssl1.1 libstdc++6 libzen0v5 zlib1g apt-transport-https')\n",
        "    runSh('sudo curl -sL -o /var/cache/apt/archives/MEGAcmd.deb https://mega.nz/linux/MEGAsync/Debian_9.0/amd64/megacmd-Debian_9.0_amd64.deb', output=True)\n",
        "    runSh('sudo dpkg -i /var/cache/apt/archives/MEGAcmd.deb', output=True)\n",
        "    print(\"MEGA is installed.\")\n",
        "    clear_output()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Pi_Rk09cuvQ"
      },
      "source": [
        "### provide URL and Mega ID\n",
        "\n",
        "Add the URL to fetch from Mega, and the username and password for those files if it wasn't a public URL and to use the pro mega quota if you have a pro account."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9jy2wxACPmn"
      },
      "outputs": [],
      "source": [
        "#It's optional to provide the MEGA username and password, it's used for giving more download quota if you have a MEGA pro account. \n",
        "MEGA_USERNAME = \"\"  #optional \n",
        "MEGA_PASSWORD = \"\"  #optional \n",
        "\n",
        "TAGGED_DATASET_URL = \"https://mega.nz/file/kc40hbgL#WH0rLmRwkJodzD4yazWWAlnmp_IJQiLG0jx2hhDJLhY\"\n",
        "OTHER_DATASET_URL  = \"https://mega.nz/file/MNo0ABSB#8rDqevxdQmtaNFKjQ3RS9v2pF-jL8xzmM9LLxIfAhG0\"\n",
        "OUTPUT_PATH = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjz0-BUHcuvS",
        "outputId": "d59e26fd-0be2-4da3-e546-458c74a1a20d"
      },
      "outputs": [],
      "source": [
        "# Unix, Windows and old Macintosh end-of-line\n",
        "newlines = ['\\n', '\\r\\n', '\\r']\n",
        "\n",
        "def latest_file(folder):\n",
        "  list_of_files = glob.glob(f'{folder}/*') # * means all \n",
        "  latest_file = max(list_of_files, key=os.path.getctime)\n",
        "  return latest_file\n",
        "\n",
        "def unbuffered(proc, stream='stdout'):\n",
        "    stream = getattr(proc, stream)\n",
        "    with contextlib.closing(stream):\n",
        "        while True:\n",
        "            out = []\n",
        "            last = stream.read(1)\n",
        "            # Don't loop forever\n",
        "            if last == '' and proc.poll() is not None:\n",
        "                break\n",
        "            while last not in newlines:\n",
        "                # Don't loop forever\n",
        "                if last == '' and proc.poll() is not None:\n",
        "                    break\n",
        "                out.append(last)\n",
        "                last = stream.read(1)\n",
        "            out = ''.join(out)\n",
        "            yield out\n",
        "\n",
        "\n",
        "def transfer(url):\n",
        "    import codecs\n",
        "    decoder = codecs.getincrementaldecoder(\"UTF-8\")()\n",
        "    cmd = [\"mega-get\", url, OUTPUT_PATH]\n",
        "    proc = subprocess.Popen(\n",
        "        cmd,\n",
        "        stdout=subprocess.PIPE,\n",
        "        stderr=subprocess.STDOUT,\n",
        "        # Make all end-of-lines '\\n'\n",
        "        universal_newlines=True,\n",
        "    )\n",
        "    for line in unbuffered(proc):\n",
        "        print(line)\n",
        "        \n",
        "if not OUTPUT_PATH:\n",
        "  os.makedirs(\"downloads\", exist_ok=True)\n",
        "  OUTPUT_PATH = \"downloads\"\n",
        "\n",
        "\n",
        "class TimeoutError(Exception):\n",
        "    pass\n",
        "\n",
        "def timeout(seconds=10, error_message=os.strerror(errno.ETIME)):\n",
        "    def decorator(func):\n",
        "        def _handle_timeout(signum, frame):\n",
        "            raise TimeoutError(error_message)\n",
        "\n",
        "        def wrapper(*args, **kwargs):\n",
        "            signal.signal(signal.SIGALRM, _handle_timeout)\n",
        "            signal.alarm(seconds)\n",
        "            try:\n",
        "                result = func(*args, **kwargs)\n",
        "            finally:\n",
        "                signal.alarm(0)\n",
        "            return result\n",
        "\n",
        "        return wraps(func)(wrapper)\n",
        "\n",
        "    return decorator\n",
        "\n",
        "\n",
        "@timeout(10)\n",
        "def runShT(args):\n",
        "    return runSh(args, output=True)\n",
        "\n",
        "def login(): \n",
        "    runShT(f\"mega-login {MEGA_USERNAME} {MEGA_PASSWORD}\")\n",
        "\n",
        "#if the username and password provided then login to MEGA. \n",
        "if MEGA_USERNAME != \"\" and MEGA_PASSWORD != \"\":\n",
        "    try:\n",
        "        login()\n",
        "    except TimeoutError:\n",
        "        runSh('mega-whoami', output=True)\n",
        "else:\n",
        "    print(\"Please Input your Mega IDs.\")\n",
        "\n",
        "transfer(TAGGED_DATASET_URL)\n",
        "tagged_dataset_path = latest_file('./downloads')\n",
        "transfer(OTHER_DATASET_URL)\n",
        "other_dataset_path = latest_file('./downloads')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFI20pLVcuvU"
      },
      "source": [
        "# Fetch & Clone Repo\n",
        "\n",
        "Clone the `image-tagging-tools` repo as it has all the required utils and codes from preprocessing the image datasets to training the models and using them to classify the images. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6n6QEB49cuvV",
        "outputId": "694d6054-03f6-4fda-9836-cc6dad61811b"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/kk-digital/image-tagging-tools.git\n",
        "%cd image-tagging-tools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hYcmeZicuvW"
      },
      "source": [
        "# Preprocess the dataset images (Stage 1)\n",
        "\n",
        "Use the `image-dataset-processor` utils from the previously cloned repo to process a directory of images (paths to directory of images or an archived dataset), and computes the images metadata along with its CLIP embeddings and writes the result into a JSON file into `output_folder`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guwuN0UncuvW"
      },
      "source": [
        "### Install requirements "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86S-QyvSnwG8"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%pip install ascii_graph open_clip_torch patool fire"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiqXMqOEcuvX"
      },
      "source": [
        "### Import the module/Utility "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "usaH8e-fcuvY"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.insert(0, './stage1/')\n",
        "sys.path.insert(0, './stage2/')\n",
        "sys.path.insert(0, './stage3/')\n",
        "sys.path.insert(0, './stage4/')\n",
        "from ImageDatasetProcessor import ImageDatasetProcessor\n",
        "from classify import main as classify_main\n",
        "from classify_zip import main as classify_main_zip\n",
        "from classify_zip import zip_gen as zip_generator\n",
        "from train import main as train_main\n",
        "from classify_helper_functions import *\n",
        "import patoolib\n",
        "import shutil"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "LeIimd1gcuvY"
      },
      "source": [
        "### Set required variables by the utility\n",
        "\n",
        "Initialize the required parameters needed by the dataset preprocessor utility and they are described as follows: \n",
        "\n",
        "* `input_folder` _[str]_ -  path to the directory containing sub-folders of each tag.\n",
        "* `output_folder` _[str]_ - path to the directory where to save the files into it.\n",
        "\n",
        "* `clip_model` _[str]_ - CLIP model to be used\n",
        "\n",
        "* `pretrained` _[str]_ - the pre-trained model to be used for CLIP\n",
        "* `batch_size` _[int]_ -  number of images to process at a time\n",
        "* `num_threads` _[int]_ - the number to be used in this process\n",
        "\n",
        "* `device` _[str]_ -  the device to be used in computing the CLIP embeddings, if `None` is provided then `cuda` will be used if available\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sFReXmwmcuvY"
      },
      "outputs": [],
      "source": [
        "dataset_path = '../drive/MyDrive/dataset/subset'\n",
        "output_folder = './output'\n",
        "tagged_dataset = True\n",
        "clip_model = 'ViT-B-32'\n",
        "pretrained = 'openai'\n",
        "batch_size = 32\n",
        "num_threads = 4\n",
        "device = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D691ZEnrcuvZ"
      },
      "source": [
        "### Run the Preprocessor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZPLvQ1EcuvZ",
        "outputId": "314a1c84-f94f-4513-ad23-98f94cf03c3a"
      },
      "outputs": [],
      "source": [
        "ImageDatasetProcessor.process_dataset(\n",
        "    dataset_path, \n",
        "    output_folder,\n",
        "    tagged_dataset, \n",
        "    clip_model, \n",
        "    pretrained,\n",
        "    batch_size, \n",
        "    num_threads, \n",
        "    device\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training the Classifiers (Stage 2)\n",
        "Given a `metadata` json file containing embeddings for images and `tag-to-image-hash` json file containing images' hash with tags, the script start to make for every tag two binary classification models and save it in output folder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ld8q1avSOdOP"
      },
      "source": [
        "### Train script variables\n",
        "\n",
        "\n",
        "* `metadata_json` _[string]_ - _[required]_ - The path to the metadata json file. \n",
        "* `tag_to_hash_json` _[string]_ - _[required]_ - The path to tag-to-hash json file. \n",
        "\n",
        "* `output` _[string]_ - _[optional]_ - The path to the output directory.\n",
        "* `test_per` _[float]_ - _[optional]_ - The percentage of the test images from the dataset, default = 0.1 \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fYigdGm_PHLm"
      },
      "outputs": [],
      "source": [
        "# Run from ./image-tagging-tools directory\n",
        "metadata_json = './output/input-metadata.json' \n",
        "tag_to_hash_json = './output/input-tag-to-image-hash-list.json'\n",
        "output_dir = './output'\n",
        "test_per = 0.1"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8GLoJWDJS1vm"
      },
      "source": [
        "### Run the training script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjWOmPV1-3Zz",
        "outputId": "8f7599aa-1beb-4535-a2c4-21021cc1692e"
      },
      "outputs": [],
      "source": [
        "train_main(\n",
        "    metadata_json = metadata_json,\n",
        "    tag_to_hash_json = tag_to_hash_json,\n",
        "    output_dir = output_dir,\n",
        "    test_per = test_per\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Listing all the models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "list_models('./output/models') # listing all the models we have for classification. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtcQghX_S5YD"
      },
      "source": [
        "### Classification script variables\n",
        "\n",
        "* `directory` _[string]_ - _[required]_ - The path to the images' folder or images' .zip file. \n",
        "* `metadata_json` _[string]_ - _[required]_ - The path to the metadata json file for CLIP embeddings. \n",
        "* `output` _[string]_ - _[optional]_ - The path to the output directory for the inference results. \n",
        "* `model` _[string]_ - _[optional]_ - The path to the models' .pkl files directory or single .pkl file model.\n",
        "* `output_bins` _[int]_ - _[optional]_ -  The number of bins of the results for each model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWUepQsbWrT1"
      },
      "source": [
        "#### Classification fot other-validation folder (pre-computed CLIP embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wvcwd-mVQ_A8"
      },
      "outputs": [],
      "source": [
        "folder_path    = os.path.join(tagged_images_folder,\"other-validation\")\n",
        "output_dir     = \"./classification_other_validation\"\n",
        "json_file_path =  \"./output/input-metadata.json\"\n",
        "bins_number    = 10\n",
        "model_path     = \"./output/models\" "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hj7cPMRThnLC",
        "outputId": "df64e9ed-cc42-4e3f-f741-a12bfb50da60"
      },
      "outputs": [],
      "source": [
        "classify_main(\n",
        "        folder_path    = folder_path , \n",
        "        output_dir     = output_dir, \n",
        "        json_file_path = json_file_path, \n",
        "        bins_number    = bins_number , \n",
        "        model_path     = model_path, \n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Single Image -- Single Model Classification Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Image file to be classified \n",
        "folder_path    = \"/content/downloads/pixel-art-tagged-v2/other-validation/https___i.pinimg.com_originals_00_79_2c_00792c0d83d415a707bdacee51646d9e.png\"\n",
        "output_dir     = \".output/classification_single_image_single_model\"\n",
        "json_file_path = \".output/input-metadata.json\"\n",
        "bins_number    = 10\n",
        "model_path     = \"./output/models/model-ovr-logistic-regression-tag-not-pixel-art-digital.pkl\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "classify_main(\n",
        "        folder_path    = folder_path , \n",
        "        output_dir     = output_dir, \n",
        "        json_file_path = json_file_path, \n",
        "        bins_number    = bins_number, \n",
        "        model_path     = model_path, \n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Single image -- All the models classification example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Image file to be classified \n",
        "folder_path    = \"/content/downloads/pixel-art-tagged-v2/other-validation/https___i.pinimg.com_originals_00_79_2c_00792c0d83d415a707bdacee51646d9e.png\"\n",
        "output_dir     = \"./output/classification_single_image_all_models\"\n",
        "json_file_path = \"./output/input-metadata.json\"\n",
        "bins_number    = 10\n",
        "model_path = \"./output/models\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "classify_main(\n",
        "        folder_path    = folder_path , \n",
        "        output_dir     = output_dir, \n",
        "        json_file_path = json_file_path, \n",
        "        bins_number    = bins_number, \n",
        "        model_path     = model_path, \n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model and Tag name Classification for Single Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TAG_NAME   = 'not-pixel-art' # tag which you want to classify.\n",
        "MODEL_TYPE = 'ovr-logistic-regression' # model type you want to use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Image file to be classified \n",
        "folder_path    = \"/content/downloads/pixel-art-tagged-v2/other-validation/https___i.pinimg.com_originals_00_79_2c_00792c0d83d415a707bdacee51646d9e.png\"\n",
        "output_dir     = \".output/classification_single_image_custom_model\"\n",
        "json_file_path = \"./output/input-metadata.json\"\n",
        "bins_number    = 10\n",
        "model_path = generate_model_path(\n",
        "                                  './output/models',\n",
        "                                  model_type= MODEL_TYPE,\n",
        "                                  tag_name= TAG_NAME\n",
        "                                 )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "classify_main(\n",
        "        folder_path    = folder_path, \n",
        "        output_dir     = output_dir, \n",
        "        json_file_path = json_file_path, \n",
        "        bins_number    = bins_number, \n",
        "        model_path     = model_path, \n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nsNclcPX49V"
      },
      "source": [
        "#### Classifcation for .zip file full of images (takes more time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDRrhjs4wTkw"
      },
      "outputs": [],
      "source": [
        "folder_path    = 'dataset/subset_3.zip'\n",
        "output_dir     = './output'\n",
        "json_file_path =  \"./output/input-metadata.json\"\n",
        "bins_number    = 5\n",
        "model_path     = \"./output/models\" "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AwyCnb0dXSGc"
      },
      "outputs": [],
      "source": [
        "classify_main_zip(\n",
        "        folder_path    = folder_path , \n",
        "        output_dir     = output_dir, \n",
        "        json_file_path = json_file_path, \n",
        "        bins_number    = bins_number , \n",
        "        model_path     = model_path, \n",
        "        )"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Get Score for Single Tag from Images Data (in ZIP Archive)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sys.path.insert(0, '../image-tagging-tools/stage4/')\n",
        "from classify_zip import zip_gen\n",
        "from classify_zip_helper_functions import get_single_tag_score, get_clip, create_models_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "folder_path    = './dataset/subset_3.zip'\n",
        "model_path     = './output/models/model-ovr-svm-tag-pos-video-game-side-scrolling.pkl'\n",
        "th_score       = 0.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def any_function_to_run(score):\n",
        "    '''Function to run when certain prob_score is met'''\n",
        "    pass    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "clip_model , preprocess , device = get_clip(clip_model_type= 'ViT-B-32',pretrained= 'openai')\n",
        "model_dict = create_models_dict(model_path)\n",
        "print (model_dict)\n",
        "\n",
        "# Loop through each zip file.\n",
        "for file in [folder_path]:\n",
        "    # Generating images\n",
        "    for img, img_file_name in zip_gen(file):\n",
        "        # Calculate score\n",
        "        score = get_single_tag_score(img, img_file_name, model_dict, clip_model, preprocess, device)\n",
        "        print (f'[INFO] Score: {score}')\n",
        "        if th_score < score:\n",
        "            any_function_to_run(score)\n",
        "\n",
        "print(\"[INFO] Finished.\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Get Files List from Folder or ZIP Archive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from zipfile import ZipFile\n",
        "\n",
        "folder_path    = './dataset/subset'\n",
        "\n",
        "def fetch_file_paths(data_file):\n",
        "    '''Yielding contained file paths in data_file'''\n",
        "\n",
        "    if data_file.endswith('.zip'):\n",
        "\n",
        "        # Selected data_dir is a zip archive\n",
        "        with ZipFile(data_file) as archive:\n",
        "            '''Getting archive details'''\n",
        "            # Listing content\n",
        "            entries = archive.infolist()\n",
        "\n",
        "            for entry in entries:\n",
        "                # Do for every content in the zip file\n",
        "                if not entry.is_dir():\n",
        "                    \n",
        "                    with archive.open(entry) as file:\n",
        "\n",
        "                        if entry.filename.lower().endswith(('.zip')):\n",
        "                            # Another zip file found in the content.\n",
        "                            with ZipFile(file) as sub_archive:\n",
        "                                '''Getting archive details'''\n",
        "                                sub_entries = sub_archive.infolist()\n",
        "                                for sub_entry in sub_entries:\n",
        "                                    with sub_archive.open(sub_entry) as sub_file:\n",
        "                                        img_file_name = f'{data_file}/{sub_entry.filename}'\n",
        "                                        yield (img_file_name)\n",
        "\n",
        "                        else:\n",
        "                            # Should be image file.\n",
        "                            img_file_name = entry.filename\n",
        "                            yield (img_file_name)\n",
        "    else:\n",
        "        # Should be image file\n",
        "        yield data_file\n",
        "\n",
        "def get_file_paths(data_dir):\n",
        "\n",
        "    # Placeholder for file in data_dir\n",
        "    dir_list = []\n",
        "\n",
        "    if not os.path.isfile(data_dir):\n",
        "        # A normal directory\n",
        "        for root, dirs, files in os.walk(data_dir):\n",
        "            for file in files:\n",
        "                dir_list.append(os.path.join(root, file))\n",
        "    else:\n",
        "        # A single file (could be a zip archive or image)\n",
        "        dir_list = [data_dir]\n",
        "\n",
        "    # Placeholder for file_path for files found\n",
        "    file_path_list = []\n",
        "\n",
        "    for file in dir_list:\n",
        "        for file_path in fetch_file_paths(file):\n",
        "            print (f'File: {file_path}')\n",
        "            file_path_list.append(file_path)\n",
        "    \n",
        "    return file_path_list\n",
        "\n",
        "get_file_paths(folder_path)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Get List of File Hash_ID from Tag Cache Based on List of Models or Specific Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from cache_tag import TagCache\n",
        "from api_model import ModelApi\n",
        "\n",
        "# Specify path to tag cache file\n",
        "tag_cache_path = './output/tag_cache.sqlite'\n",
        "# Specify path to directory containing model pickle files or specific model pickle file\n",
        "models_path = './output/models/'\n",
        "\n",
        "# Output placeholder\n",
        "model_tag_cache_pair = {}\n",
        "\n",
        "try:\n",
        "    # Create tag cache object\n",
        "    tag_cache = TagCache()\n",
        "    # Create model api object\n",
        "    model_api=ModelApi()\n",
        "\n",
        "    # Getting models\n",
        "    ret, models_dict = model_api.get_models_dict(models_path=models_path)\n",
        "    '''\n",
        "    Example stucture of models_dict\n",
        "    {<model_name>: \n",
        "        {'classifier' : <model object>,\n",
        "        'model_type' : <model type string>,\n",
        "        'train_start_time' : <traing start time datetime object>\n",
        "        'tag' : <tag string>\n",
        "        }\n",
        "    }\n",
        "    '''\n",
        "\n",
        "    # Get tags from models_dict\n",
        "    for model in models_dict:\n",
        "        # Get list of images (hash_IDs) based on each model's tag name\n",
        "        hash_ids = tag_cache.get_hash_by_tag(db_path = tag_cache_path, tag = models_dict[model]['tag'])\n",
        "        # Append list of hash IDs to the result dict\n",
        "        model_tag_cache_pair[model] = hash_ids\n",
        "    \n",
        "    # Output\n",
        "    print(json.dumps(model_tag_cache_pair, indent=2))\n",
        "\n",
        "except Exception as e:\n",
        "    print (f'[ERROR] {e}: Getting data from tag cache failed')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.4 (tags/v3.9.4:1f2e308, Apr  6 2021, 13:40:21) [MSC v.1928 64 bit (AMD64)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "c518711542db1b7752d0b1005bb6b1db13084b2ce60111ae8895922158ddc3d4"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
